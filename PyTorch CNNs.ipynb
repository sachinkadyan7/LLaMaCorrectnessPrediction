{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8fb0b4-a0fd-4489-8b07-acda5f4e0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision.models import convnext\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2001f3-8a56-410d-a76b-3f141a2876d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.convnext_tiny(weights=convnext.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90b045-c306-4aa1-b85e-94f60ed57587",
   "metadata": {},
   "source": [
    "## Experiment: Classify a single layer through all its 32 heads' attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62571827-054e-439c-a3ac-b5afcf741ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "LAYERS_INCLUDED = [1, 11, 21, 31]\n",
    "NUM_LAYER_LLM = len(LAYERS_INCLUDED)\n",
    "\n",
    "class AttentionMapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_file, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files) * NUM_LAYER_LLM\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx = int(idx // NUM_LAYER_LLM)\n",
    "        pt_path = self.root / self.files['filename'][f_idx]\n",
    "        f = torch.load(pt_path)\n",
    "        random_layer = idx % NUM_LAYER_LLM\n",
    "        layer_idx = LAYERS_INCLUDED[random_layer]\n",
    "        heads = f[layer_idx]\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_label = torch.tensor(random_layer)\n",
    "        heads.unsqueeze(dim=0)\n",
    "        bucket_label.unsqueeze(dim=0)\n",
    "        return heads.to(torch.float32), bucket_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e40c61-cb11-4662-8867-f995531b21c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5552, 1388)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "dataset = AttentionMapDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_list.txt\", transform=transform)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "indices = torch.arange(len(dataset)).tolist()\n",
    "split_idx = int((len(indices) // NUM_LAYER_LLM) * 0.8) * NUM_LAYER_LLM\n",
    "train_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "test_data = torch.utils.data.Subset(dataset, indices[split_idx:])\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a31d705-f1b8-4ae7-b1c1-ed0e507d10ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5552"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091aa12d-e065-4447-8f1b-230bc6a1c52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\3130354512.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [-1.1953,  1.1875, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 0.9375, -1.9062, -1.0234,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 0.3750, -1.9922, -2.0000,  ..., -0.8203, -2.0000, -2.0000],\n",
       "          [ 1.0625, -2.0000, -2.0000,  ..., -1.9141, -1.1875, -2.0000],\n",
       "          [-0.2578, -2.0000, -2.0000,  ..., -1.8125, -1.2734, -0.7734]],\n",
       " \n",
       "         [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.8906, -1.8906, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.9688, -1.9844, -1.9844,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 0.9062, -1.9609, -1.9766,  ..., -1.8984, -2.0000, -2.0000],\n",
       "          [ 0.4062, -1.9844, -1.9922,  ..., -1.9375, -1.9375, -2.0000],\n",
       "          [ 0.2812, -1.9844, -1.9922,  ..., -1.9531, -1.9531, -1.8750]],\n",
       " \n",
       "         [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [-2.0000,  2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [-2.0000, -2.0000,  2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ...,  1.9688, -2.0000, -2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.9844,  1.9531, -2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.9844,  1.9844]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.1875, -1.1797, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.2812, -1.8750, -1.3984,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 0.5625, -2.0000, -1.9844,  ..., -0.9453, -2.0000, -2.0000],\n",
       "          [ 0.0156, -2.0000, -1.9922,  ..., -1.7109, -0.6562, -2.0000],\n",
       "          [ 0.1094, -2.0000, -1.9922,  ..., -1.5938, -1.8359, -1.1641]],\n",
       " \n",
       "         [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.8125, -1.8125, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 0.5000, -1.8203, -0.6719,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [-0.5469, -1.9844, -1.9141,  ..., -1.4688, -2.0000, -2.0000],\n",
       "          [-0.7188, -1.9922, -1.9844,  ..., -1.3750, -0.8438, -2.0000],\n",
       "          [-1.1719, -1.9922, -1.9766,  ..., -1.4688, -1.3594, -1.1875]],\n",
       " \n",
       "         [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 0.7500, -0.7500, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.2188, -1.7188, -1.5078,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [-0.2188, -1.9766, -1.9922,  ..., -0.6562, -2.0000, -2.0000],\n",
       "          [-0.6250, -1.9844, -1.9922,  ..., -0.7969, -1.1250, -2.0000],\n",
       "          [-0.1328, -2.0000, -1.9922,  ..., -1.7500, -1.7891, -0.7344]]],\n",
       "        device='cuda:0'),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[split_idx+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad8b5043-3d2c-4aa5-bec5-b36d38dc5f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\3130354512.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2500, -1.9922, -1.7188,  ..., -1.9688, -1.9062, -1.9375],\n",
       "          [ 0.1719, -1.9922, -1.8125,  ..., -1.8438, -1.7109, -2.0000],\n",
       "          [-0.0234, -1.9766, -1.5000,  ..., -1.9375, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 0.7656, -1.8984, -0.8594,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.4531, -1.4531, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         [[-0.4844, -2.0000, -2.0000,  ..., -1.8906, -1.7656,  0.0938],\n",
       "          [-1.1250, -2.0000, -2.0000,  ..., -1.8906,  1.0000, -2.0000],\n",
       "          [-1.2500, -2.0000, -2.0000,  ...,  1.1562, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [-0.4922, -1.7500,  0.2344,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 0.7188, -0.7188, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         [[-1.2734, -2.0000, -1.9922,  ..., -1.5859, -1.4062, -1.3281],\n",
       "          [-0.7266, -2.0000, -1.9922,  ..., -1.4141, -0.8359, -2.0000],\n",
       "          [-0.4531, -1.9766, -1.8984,  ..., -1.6328, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 0.5000, -1.8203, -0.6719,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.8125, -1.8125, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.7656, -1.9922, -2.0000,  ..., -1.9844, -1.9766, -1.8828],\n",
       "          [ 1.8906, -2.0000, -2.0000,  ..., -1.9766, -1.9688, -2.0000],\n",
       "          [ 1.8125, -2.0000, -1.9922,  ..., -1.9297, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 1.7969, -1.9531, -1.8438,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 0.7031, -0.7031, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         [[ 1.2812, -1.9844, -1.9922,  ..., -1.9922, -1.9922, -1.8984],\n",
       "          [ 1.5312, -1.9922, -1.9844,  ..., -1.9922, -1.9922, -2.0000],\n",
       "          [ 1.2656, -1.9297, -1.9688,  ..., -1.8984, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 1.7500, -1.9297, -1.8125,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.6719, -1.6797, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         [[ 0.4688, -1.9922, -1.9922,  ..., -1.9688, -1.9766, -1.9297],\n",
       "          [ 0.2812, -1.9922, -1.9922,  ..., -1.9609, -1.9531, -2.0000],\n",
       "          [ 0.5938, -1.9766, -1.9844,  ..., -1.9375, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 1.9688, -1.9844, -1.9844,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.8906, -1.8906, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]]],\n",
       "        device='cuda:0'),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3374f21f-7889-4b6c-992f-199a74864dd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\3130354512.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.8906, -1.9844, -1.9766,  ..., -2.0000, -2.0000, -1.3047],\n",
       "          [-2.0000, -1.3672, -1.9297,  ..., -1.9922, -1.9922,  0.4844],\n",
       "          [-2.0000, -2.0000, -1.4219,  ..., -1.9922, -1.9688,  0.8750],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.8984, -1.9062,  1.8125],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000,  0.8438, -0.8438],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[ 0.0156, -1.8984, -1.9766,  ..., -2.0000, -2.0000, -0.2422],\n",
       "          [-2.0000, -0.0156, -1.8828,  ..., -2.0000, -2.0000, -0.2422],\n",
       "          [-2.0000, -2.0000, -1.0312,  ..., -2.0000, -2.0000,  0.9062],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.0781, -1.9062,  0.9844],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -0.4844,  0.4844],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.5625, -1.7500, -1.8594,  ..., -2.0000, -2.0000,  0.9688],\n",
       "          [-2.0000, -1.1094, -1.5781,  ..., -2.0000, -2.0000,  0.4219],\n",
       "          [-2.0000, -2.0000, -1.5391,  ..., -1.9922, -2.0000,  1.1406],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.6016, -1.8750,  1.4844],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.6875,  1.6875],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.7188, -1.8125, -1.5859,  ..., -1.9922, -2.0000, -0.2969],\n",
       "          [-2.0000, -0.6875, -1.5703,  ..., -1.9844, -2.0000, -0.2422],\n",
       "          [-2.0000, -2.0000, -0.3750,  ..., -1.9844, -1.9922, -0.0078],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.3750, -1.8750,  1.2500],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.1719,  1.1719],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.9062, -1.9688, -1.9609,  ..., -1.9844, -1.9844,  1.0469],\n",
       "          [-2.0000, -1.9844, -1.9766,  ..., -1.9922, -1.9922,  1.2812],\n",
       "          [-2.0000, -2.0000, -1.9375,  ..., -1.9844, -1.9766,  1.2500],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.9766, -1.9609,  1.9375],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.9141,  1.9219],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.9062, -1.9688, -1.9688,  ..., -1.9922, -1.9922, -0.5000],\n",
       "          [-2.0000, -1.9609, -1.9688,  ..., -1.9922, -1.9922, -0.5859],\n",
       "          [-2.0000, -2.0000, -1.9531,  ..., -1.9766, -1.9688,  0.0938],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.9844, -1.9844,  1.9688],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.8906,  1.8906],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]]],\n",
       "        device='cuda:0'),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f0e4ed-1a3b-47e9-b393-60f3923da47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33cc6836-2872-45ec-821d-2262745170bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(32, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change features\n",
    "IN_CHANNELS = 32\n",
    "NUM_CLASSES = NUM_LAYER_LLM\n",
    "device = 'cuda'\n",
    "\n",
    "accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "auroc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "model.features[0][0] = nn.Conv2d(IN_CHANNELS, 96, kernel_size=(4,4), stride=(4,4))\n",
    "model.classifier[2] = nn.Linear(768, NUM_CLASSES)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98980bd8-118b-44b8-81b4-d496e10da442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\3130354512.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5552/5552 [02:52<00:00, 32.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 1.639172484960985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1388/1388 [00:20<00:00, 67.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.2500, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.2500, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.2500, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5552/5552 [02:41<00:00, 34.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 1.4343337866152912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1388/1388 [00:20<00:00, 66.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.2500, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.2500, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.2500, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5552/5552 [02:46<00:00, 33.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 1.396711540131988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1388/1388 [00:20<00:00, 66.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: tensor(0.2500, device='cuda:0')\n",
      "Epoch 2: Validation Precision: tensor(0.2500, device='cuda:0')\n",
      "Epoch 2: Validation Recall: tensor(0.2500, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        # val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    # print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28b88c-7c29-46a0-8b62-1429047a8ee5",
   "metadata": {},
   "source": [
    "## Experiment: Classify Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e436b9ee-4676-44b1-8ea2-0ab2159d7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "LAYERS_INCLUDED = [1, 11, 21, 31]\n",
    "NUM_LAYER_LLM = len(LAYERS_INCLUDED)\n",
    "\n",
    "class AttentionMapCorrectnessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_df, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = annotations_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx = int(idx)\n",
    "        pt_path = self.root / self.files['filename'][f_idx]\n",
    "        f = torch.load(pt_path)\n",
    "        random_layer = 3\n",
    "        layer_idx = LAYERS_INCLUDED[random_layer]  ## We can select the specific layer on which we want to test correctness.\n",
    "        heads = f[layer_idx]\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_label = torch.tensor(self.files['prediction'][f_idx] == self.files['correct'][f_idx], dtype=torch.long)\n",
    "        heads.unsqueeze(dim=0)\n",
    "        bucket_label.unsqueeze(dim=0)\n",
    "        return heads.to(torch.float32), bucket_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d42f7f21-f148-4b89-8d60-f34a327bacd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>prediction</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1435</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1518</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1619</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>449</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\399_attent...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\1053_atten...</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>361</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\319_attent...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>455</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\403_attent...</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>895</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\7_attentio...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>798</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\712_attent...</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>533</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\474_attent...</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1735 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           filename prediction  \\\n",
       "0      1435  auxiliary_train\\science_elementary\\attentions\\...          A   \n",
       "1      1518  auxiliary_train\\science_elementary\\attentions\\...          C   \n",
       "2      1619  auxiliary_train\\science_elementary\\attentions\\...          B   \n",
       "3       449  auxiliary_train\\arc_hard\\attentions\\399_attent...          B   \n",
       "4        59  auxiliary_train\\arc_hard\\attentions\\1053_atten...          C   \n",
       "...     ...                                                ...        ...   \n",
       "1730    361  auxiliary_train\\arc_hard\\attentions\\319_attent...          A   \n",
       "1731    455  auxiliary_train\\arc_hard\\attentions\\403_attent...          C   \n",
       "1732    895  auxiliary_train\\arc_hard\\attentions\\7_attentio...          D   \n",
       "1733    798  auxiliary_train\\arc_hard\\attentions\\712_attent...          B   \n",
       "1734    533  auxiliary_train\\arc_hard\\attentions\\474_attent...          B   \n",
       "\n",
       "     correct  \n",
       "0          A  \n",
       "1          C  \n",
       "2          D  \n",
       "3          B  \n",
       "4          A  \n",
       "...      ...  \n",
       "1730       A  \n",
       "1731       D  \n",
       "1732       D  \n",
       "1733       D  \n",
       "1734       C  \n",
       "\n",
       "[1735 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(Path.home() / \"Downloads/mmlu_attention_files_list.txt\")\n",
    "dataset_df = dataset_df.sample(frac=1).reset_index()  # shuffle the dataset\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba392b51-2709-4abb-b1b5-4cc3bed0ae8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1388, 347)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "dataset = AttentionMapCorrectnessDataset(Path.home() / \"Downloads/mmlu_output/\", dataset_df, transform=None)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "indices = torch.arange(len(dataset)).tolist()\n",
    "split_idx = int((len(indices)) * 0.8)\n",
    "train_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "test_data = torch.utils.data.Subset(dataset, indices[split_idx:])\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be54d083-e8bd-4546-8649-3132559fb94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1388"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "caa9a288-91ad-490f-bbb3-9f5127ea319b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\2840655149.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.5234e-01, 3.4570e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [5.4688e-01, 5.8838e-02, 3.9258e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [3.9453e-01, 1.5991e-02, 5.2795e-03,  ..., 3.4375e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.3145e-01, 1.4267e-03, 3.1128e-03,  ..., 1.3477e-01,\n",
       "           4.8828e-01, 0.0000e+00],\n",
       "          [2.5586e-01, 8.9264e-04, 9.8419e-04,  ..., 4.1748e-02,\n",
       "           7.5684e-02, 5.2734e-01]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.1094e-01, 2.8906e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.7109e-01, 4.2236e-02, 8.7402e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [4.9805e-01, 4.3640e-03, 1.2894e-03,  ..., 2.8320e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.4219e-01, 1.3275e-03, 1.8158e-03,  ..., 1.0156e-01,\n",
       "           7.3730e-02, 0.0000e+00],\n",
       "          [6.1719e-01, 3.1090e-04, 4.2534e-04,  ..., 2.4414e-02,\n",
       "           4.3701e-02, 2.5586e-01]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.5977e-01, 7.3828e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.5625e-01, 3.0640e-02, 3.1250e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [3.0078e-01, 1.2573e-02, 4.4250e-03,  ..., 5.4688e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [5.9766e-01, 1.7776e-03, 1.8921e-03,  ..., 7.8125e-02,\n",
       "           2.3047e-01, 0.0000e+00],\n",
       "          [5.3516e-01, 1.0529e-03, 1.3580e-03,  ..., 3.3936e-02,\n",
       "           6.4453e-02, 2.8711e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.0703e-01, 2.9297e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.9297e-01, 1.8359e-01, 2.4292e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [4.9609e-01, 1.5747e-02, 7.2937e-03,  ..., 9.9609e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [3.8672e-01, 1.2146e-02, 4.4556e-03,  ..., 1.2988e-01,\n",
       "           1.5015e-02, 0.0000e+00],\n",
       "          [4.7461e-01, 6.6833e-03, 3.6316e-03,  ..., 4.1992e-02,\n",
       "           1.0193e-02, 4.2969e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.6328e-01, 1.3477e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.7578e-01, 2.2461e-02, 3.0078e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [2.9688e-01, 6.5613e-03, 1.5527e-01,  ..., 1.0864e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [3.2031e-01, 2.1210e-03, 4.6875e-02,  ..., 2.1851e-02,\n",
       "           2.6978e-02, 0.0000e+00],\n",
       "          [3.0664e-01, 1.6632e-03, 7.3242e-02,  ..., 5.7983e-03,\n",
       "           1.5503e-02, 1.3855e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.7930e-01, 7.1875e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.7500e-01, 4.8340e-02, 7.5195e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [6.8359e-01, 1.8555e-02, 1.0437e-02,  ..., 5.8350e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.4844e-01, 6.7139e-03, 4.5471e-03,  ..., 3.1006e-02,\n",
       "           9.2285e-02, 0.0000e+00],\n",
       "          [6.1328e-01, 6.4697e-03, 3.7994e-03,  ..., 3.2227e-02,\n",
       "           8.2031e-02, 5.3955e-02]]], device='cuda:0'),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[split_idx-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a90ee1c7-a1ac-400c-b575-04cbeef9d14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\2840655149.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.5625e-01, 3.4180e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [5.5859e-01, 5.2979e-02, 3.8867e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [4.0234e-01, 1.1230e-02, 2.7161e-03,  ..., 3.7695e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.4805e-01, 7.9727e-04, 5.0354e-03,  ..., 1.4062e-01,\n",
       "           5.3125e-01, 0.0000e+00],\n",
       "          [2.7734e-01, 8.7357e-04, 4.0283e-03,  ..., 5.5176e-02,\n",
       "           6.9824e-02, 5.1953e-01]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.1484e-01, 2.8516e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.7891e-01, 3.8574e-02, 8.2520e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [5.1562e-01, 6.8359e-03, 1.2054e-03,  ..., 2.8906e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.8906e-01, 9.9182e-04, 9.9182e-04,  ..., 9.5703e-02,\n",
       "           6.2500e-02, 0.0000e+00],\n",
       "          [5.9766e-01, 5.1117e-04, 9.2316e-04,  ..., 3.1006e-02,\n",
       "           4.0771e-02, 2.9102e-01]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.5391e-01, 7.4609e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.6016e-01, 2.8442e-02, 3.1055e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [2.8125e-01, 4.2725e-03, 3.2806e-03,  ..., 5.5078e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.0547e-01, 2.1057e-03, 1.5411e-03,  ..., 6.4941e-02,\n",
       "           2.5195e-01, 0.0000e+00],\n",
       "          [5.3516e-01, 1.0681e-03, 2.6398e-03,  ..., 3.1494e-02,\n",
       "           7.3242e-02, 2.7344e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.1094e-01, 2.8711e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.0078e-01, 1.7578e-01, 2.3560e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [2.8125e-01, 1.3855e-02, 4.2725e-03,  ..., 4.3457e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [4.2578e-01, 1.1475e-02, 3.9368e-03,  ..., 1.2500e-01,\n",
       "           1.9165e-02, 0.0000e+00],\n",
       "          [3.6719e-01, 5.9814e-03, 2.8992e-03,  ..., 3.3936e-02,\n",
       "           4.2114e-03, 2.5269e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.6719e-01, 1.3281e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.9141e-01, 2.4170e-02, 2.8516e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [3.4375e-01, 1.2512e-02, 2.3828e-01,  ..., 1.3550e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [5.0391e-01, 2.7771e-03, 5.6396e-02,  ..., 2.7100e-02,\n",
       "           3.8086e-02, 0.0000e+00],\n",
       "          [4.1211e-01, 3.1586e-03, 1.2891e-01,  ..., 6.0425e-03,\n",
       "           1.5503e-02, 1.3123e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.7930e-01, 7.1875e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.7891e-01, 4.9072e-02, 7.2754e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [7.2656e-01, 1.5747e-02, 8.6060e-03,  ..., 7.6172e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.8750e-01, 6.5613e-03, 6.0730e-03,  ..., 4.3213e-02,\n",
       "           6.7871e-02, 0.0000e+00],\n",
       "          [6.3672e-01, 7.2327e-03, 4.9133e-03,  ..., 4.9072e-02,\n",
       "           6.7383e-02, 5.5420e-02]]], device='cuda:0'),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02a12c06-95a8-46fd-bc4c-9e432036fab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\2840655149.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.5625e-01, 3.4180e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [5.5469e-01, 5.4199e-02, 3.9062e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [4.5312e-01, 4.7913e-03, 1.5564e-03,  ..., 3.5742e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.4512e-01, 4.1008e-04, 1.8997e-03,  ..., 1.1572e-01,\n",
       "           5.6641e-01, 0.0000e+00],\n",
       "          [2.6953e-01, 5.1498e-04, 3.0060e-03,  ..., 5.4443e-02,\n",
       "           8.0078e-02, 4.6680e-01]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.1484e-01, 2.8516e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.7500e-01, 3.9551e-02, 8.4961e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [5.7031e-01, 1.7014e-03, 4.1771e-04,  ..., 2.4023e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.0469e-01, 5.4550e-04, 1.1826e-04,  ..., 7.3730e-02,\n",
       "           7.6172e-02, 0.0000e+00],\n",
       "          [6.5234e-01, 2.8801e-04, 3.1662e-04,  ..., 2.5146e-02,\n",
       "           4.4678e-02, 2.2852e-01]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.5781e-01, 7.4219e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.5625e-01, 2.9297e-02, 3.1445e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [3.3203e-01, 2.2583e-03, 8.8120e-04,  ..., 4.7656e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [5.9766e-01, 5.8365e-04, 1.4038e-03,  ..., 7.5195e-02,\n",
       "           2.5586e-01, 0.0000e+00],\n",
       "          [6.0156e-01, 3.4714e-04, 1.1063e-03,  ..., 2.4414e-02,\n",
       "           3.7598e-02, 2.6953e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.1484e-01, 2.8711e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.0078e-01, 1.7773e-01, 2.3438e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [4.6289e-01, 2.1851e-02, 9.9487e-03,  ..., 1.1377e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [4.3555e-01, 1.2939e-02, 2.6245e-03,  ..., 1.7773e-01,\n",
       "           1.5991e-02, 0.0000e+00],\n",
       "          [5.0391e-01, 1.0254e-02, 4.0283e-03,  ..., 5.1758e-02,\n",
       "           8.3008e-03, 3.4424e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.7109e-01, 1.2891e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.8359e-01, 2.3071e-02, 2.9297e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [3.9453e-01, 7.7515e-03, 1.7578e-01,  ..., 3.0396e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [5.0781e-01, 1.7471e-03, 3.4912e-02,  ..., 5.2734e-02,\n",
       "           6.2012e-02, 0.0000e+00],\n",
       "          [4.2578e-01, 2.0294e-03, 7.3730e-02,  ..., 1.4709e-02,\n",
       "           3.1250e-02, 1.3855e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.7734e-01, 7.2266e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.7500e-01, 4.9805e-02, 7.4219e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [7.4219e-01, 1.2024e-02, 7.0496e-03,  ..., 6.4453e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.6562e-01, 6.0425e-03, 4.6387e-03,  ..., 2.4414e-02,\n",
       "           3.3691e-02, 0.0000e+00],\n",
       "          [7.1484e-01, 1.0132e-02, 4.6997e-03,  ..., 3.2959e-02,\n",
       "           3.3203e-02, 3.3203e-02]]], device='cuda:0'),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6e7ddea-1f92-4e31-840a-f9a08f03d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d112bf4b-fab3-4339-8085-ae4005aba5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(32, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change features\n",
    "IN_CHANNELS = 32\n",
    "NUM_CLASSES = 2\n",
    "device = 'cuda'\n",
    "\n",
    "accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "auroc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "model = torchvision.models.convnext_tiny(weights=convnext.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "model.features[0][0] = nn.Conv2d(IN_CHANNELS, 96, kernel_size=(4,4), stride=(4,4))\n",
    "model.classifier[2] = nn.Linear(768, NUM_CLASSES)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "892e38e7-9941-49da-8d10-acb5d7d46f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\2840655149.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1388/1388 [00:42<00:00, 32.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 1.244797036087731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 347/347 [00:05<00:00, 65.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.7666, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.7666, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.7666, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1388/1388 [00:41<00:00, 33.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.6366801301050282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 347/347 [00:05<00:00, 64.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.7666, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.7666, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.7666, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1388/1388 [00:42<00:00, 32.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.6105254676396977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 347/347 [00:05<00:00, 65.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: tensor(0.7666, device='cuda:0')\n",
      "Epoch 2: Validation Precision: tensor(0.7666, device='cuda:0')\n",
      "Epoch 2: Validation Recall: tensor(0.7666, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        # val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    # print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c399d7-18ca-4f9d-8409-47186a9ff110",
   "metadata": {},
   "source": [
    "## Experiment: Classify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c6d31d64-f011-4daf-8a77-9f55c8d4f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "LAYERS_INCLUDED = [1, 11, 21, 31]\n",
    "NUM_LAYER_LLM = len(LAYERS_INCLUDED)\n",
    "\n",
    "class AttentionMapDFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_df, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = annotations_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx = int(idx)\n",
    "        pt_path = self.root / self.files['filename'][f_idx]\n",
    "        f = torch.load(pt_path)\n",
    "        random_layer = 1\n",
    "        layer_idx = LAYERS_INCLUDED[random_layer]\n",
    "        heads = f[layer_idx]\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_label = torch.tensor(self.files['dataset'][f_idx] == 'science_elementary', dtype=torch.long)\n",
    "        heads.unsqueeze(dim=0)\n",
    "        bucket_label.unsqueeze(dim=0)\n",
    "        return heads.to(torch.float32), bucket_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8380c254-a32a-439c-8a03-5826306fafe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>prediction</th>\n",
       "      <th>correct</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>757</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\676_attent...</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\1042_atten...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1582</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1484</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1121</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>897</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\801_attent...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1246</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>1101</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\986_attent...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>310</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\273_attent...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>706</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\62_attenti...</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1735 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           filename prediction  \\\n",
       "0       757  auxiliary_train\\arc_hard\\attentions\\676_attent...          D   \n",
       "1        47  auxiliary_train\\arc_hard\\attentions\\1042_atten...          B   \n",
       "2      1582  auxiliary_train\\science_elementary\\attentions\\...          C   \n",
       "3      1484  auxiliary_train\\science_elementary\\attentions\\...          B   \n",
       "4      1121  auxiliary_train\\science_elementary\\attentions\\...          B   \n",
       "...     ...                                                ...        ...   \n",
       "1730    897  auxiliary_train\\arc_hard\\attentions\\801_attent...          B   \n",
       "1731   1246  auxiliary_train\\science_elementary\\attentions\\...          A   \n",
       "1732   1101  auxiliary_train\\arc_hard\\attentions\\986_attent...          C   \n",
       "1733    310  auxiliary_train\\arc_hard\\attentions\\273_attent...          A   \n",
       "1734    706  auxiliary_train\\arc_hard\\attentions\\62_attenti...          B   \n",
       "\n",
       "     correct             dataset  \n",
       "0          C            arc_hard  \n",
       "1          B            arc_hard  \n",
       "2          C  science_elementary  \n",
       "3          B  science_elementary  \n",
       "4          B  science_elementary  \n",
       "...      ...                 ...  \n",
       "1730       B            arc_hard  \n",
       "1731       A  science_elementary  \n",
       "1732       C            arc_hard  \n",
       "1733       A            arc_hard  \n",
       "1734       D            arc_hard  \n",
       "\n",
       "[1735 rows x 5 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(Path.home() / \"Downloads/mmlu_attention_files_list.txt\")\n",
    "dataset_df['dataset'] = dataset_df['filename'].apply(lambda filename: str(Path(filename).parent.parent.name))\n",
    "dataset_es = dataset_df[dataset_df['dataset'] == 'science_elementary'].reset_index()\n",
    "dataset_df = dataset_df.sample(frac=1).reset_index()\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3e712057-98d9-4b2d-b704-6c31b6a64e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1388, 347)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "dataset = AttentionMapDFDataset(Path.home() / \"Downloads/mmlu_output/\", dataset_df, transform=transform)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "indices = torch.arange(len(dataset)).tolist()\n",
    "split_idx = int((len(indices)) * 0.8)\n",
    "train_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "test_data = torch.utils.data.Subset(dataset, indices[split_idx:])\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5f5d6b63-40cc-4a5b-9363-ed9a938e74ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1388"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c73d4f6b-1dff-4957-957f-92b41ff9c73e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\600380055.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.7500, -1.3828, -1.3438,  ..., -1.9766, -1.9844,  0.2344],\n",
       "          [-2.0000, -1.7500, -1.2500,  ..., -1.9531, -1.9531,  0.5469],\n",
       "          [-2.0000, -2.0000, -1.4609,  ..., -1.6094, -1.8750, -0.7812],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.6719, -1.2812,  0.9531],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.8047,  1.7969],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.8906, -1.9297, -1.6562,  ..., -1.9766, -1.9609, -0.0078],\n",
       "          [-2.0000, -1.8438, -1.6797,  ..., -1.9531, -1.9297,  0.1094],\n",
       "          [-2.0000, -2.0000, -1.6797,  ..., -1.8828, -1.9062, -0.2812],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.8750, -1.8828,  1.7656],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.8828,  1.8906],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.8906, -1.7734, -1.5234,  ..., -1.8125, -1.8672, -0.7109],\n",
       "          [-2.0000, -1.8750, -1.5781,  ..., -1.4297, -1.6719, -0.5312],\n",
       "          [-2.0000, -2.0000, -1.8984,  ..., -1.2656, -1.7344, -0.5000],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.9922, -1.5859,  1.5781],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.9453,  1.9531],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.9297, -1.9219, -1.4531,  ..., -1.9141, -1.8906,  0.5000],\n",
       "          [-2.0000, -1.8047, -1.4688,  ..., -1.8125, -1.8672,  0.3281],\n",
       "          [-2.0000, -2.0000, -1.9062,  ..., -1.6094, -1.6797,  0.6719],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.6719, -1.9219,  1.5938],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.9375,  1.9375],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.8438, -1.9453, -1.8359,  ..., -1.9609, -1.9375,  0.1562],\n",
       "          [-2.0000, -1.8594, -1.7344,  ..., -1.9062, -1.9141,  0.0312],\n",
       "          [-2.0000, -2.0000, -1.5625,  ..., -1.8906, -1.8359,  0.0156],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.6406, -1.8281,  1.4688],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.6562,  1.6562],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.8516, -1.9219, -1.8594,  ..., -1.9453, -1.9219,  0.1875],\n",
       "          [-2.0000, -1.9609, -1.8984,  ..., -1.8516, -1.7500,  1.1562],\n",
       "          [-2.0000, -2.0000, -1.7578,  ..., -1.8984, -1.7969,  1.0156],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.8281, -1.9609,  1.7812],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.5234,  1.5156],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]]],\n",
       "        device='cuda:0'),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[split_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "93d6c327-3d5e-40cc-b162-8fc69036182d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\600380055.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7578, -1.9766, -1.9688,  ..., -1.6797, -1.9297, -1.8984],\n",
       "          [-0.5156, -1.9375, -1.9531,  ..., -1.7500, -1.8984, -2.0000],\n",
       "          [-0.2578, -1.8359, -1.8438,  ..., -1.8750, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 1.6875, -1.9062, -1.7812,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.9219, -1.9141, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         [[-0.0781, -1.9453, -1.9531,  ..., -1.8516, -1.9219, -1.7812],\n",
       "          [ 0.6719, -1.7812, -1.8125,  ..., -1.9141, -1.9375, -2.0000],\n",
       "          [ 0.7031, -1.6875, -1.8594,  ..., -1.7656, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 1.7656, -1.9609, -1.8125,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.5156, -1.5156, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         [[ 0.5938, -1.9844, -1.9922,  ..., -1.8750, -1.7266, -1.7734],\n",
       "          [ 0.5156, -1.9844, -1.9844,  ..., -1.7344, -1.6406, -2.0000],\n",
       "          [-1.1094, -1.9922, -1.9922,  ..., -1.6484, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 1.6094, -1.9141, -1.6953,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.6406, -1.6328, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0312, -1.9062, -1.8125,  ..., -1.3750, -1.9219, -1.9453],\n",
       "          [ 0.2188, -1.8750, -1.7656,  ..., -1.6875, -1.8672, -2.0000],\n",
       "          [ 0.4844, -1.6328, -1.6562,  ..., -1.9141, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 1.6250, -1.9219, -1.7031,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.9375, -1.9297, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         [[ 1.3594, -1.9375, -1.9453,  ..., -1.9688, -1.9766, -1.9453],\n",
       "          [ 1.3125, -1.9453, -1.9453,  ..., -1.9531, -1.9609, -2.0000],\n",
       "          [ 1.1094, -1.8984, -1.9375,  ..., -1.8516, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 1.6562, -1.9219, -1.7266,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.9219, -1.9219, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         [[-0.8281, -1.9844, -1.9844,  ..., -1.8047, -1.8828, -1.9062],\n",
       "          [-0.5078, -1.9844, -1.9453,  ..., -1.8828, -1.8281, -2.0000],\n",
       "          [-0.5469, -1.9688, -1.9297,  ..., -1.9453, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [ 1.8281, -1.9531, -1.8750,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.8906, -1.8906, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000]]],\n",
       "        device='cuda:0'),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7173f392-82f3-4a70-bb2b-758a0e5edebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\600380055.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.6562,  1.6562],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.6406, -1.8281,  1.4688],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -1.4688,  ..., -1.9219, -1.8906, -0.0391],\n",
       "          [-2.0000, -1.8203, -1.7969,  ..., -1.8906, -1.8984, -0.1953],\n",
       "          [-1.8203, -1.9141, -1.8359,  ..., -1.9609, -1.9375, -0.1328]],\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.8438,  1.8438],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.6562, -1.8359,  1.5000],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -1.9062,  ..., -1.9844, -1.9844,  0.6094],\n",
       "          [-2.0000, -1.7812, -1.7734,  ..., -2.0000, -1.9844,  1.3750],\n",
       "          [-1.5469, -1.2109, -1.9219,  ..., -2.0000, -1.9922,  0.5938]],\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.8594,  1.8594],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.8125, -1.9375,  1.7500],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -1.3984,  ..., -1.9453, -1.9531,  0.1719],\n",
       "          [-2.0000, -1.8594, -1.8672,  ..., -1.9844, -1.9844, -0.2500],\n",
       "          [-1.9141, -1.9453, -1.6797,  ..., -1.9922, -1.9844,  0.1562]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.6406,  1.6406],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.6953, -1.9141,  1.6094],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -0.6562,  ..., -1.9922, -1.9922, -1.1406],\n",
       "          [-2.0000, -1.5312, -1.4141,  ..., -1.9766, -1.9766,  0.2969],\n",
       "          [-1.7266, -1.7188, -1.6328,  ..., -1.9922, -1.9844,  0.4844]],\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.8438,  1.8438],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.9375, -1.9531,  1.8906],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -1.7656,  ..., -1.9844, -1.9922, -0.7422],\n",
       "          [-2.0000, -1.9766, -1.8281,  ..., -1.9609, -1.9922, -0.1953],\n",
       "          [-1.9609, -1.9766, -1.8359,  ..., -1.9688, -1.9844, -0.0156]],\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.6250,  1.6250],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.1094, -1.6875,  0.7969],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -1.2656,  ..., -1.9609, -1.9844, -0.9688],\n",
       "          [-2.0000, -1.4844, -1.6016,  ..., -1.9844, -1.9609,  0.8750],\n",
       "          [-1.3125, -1.2188, -1.7969,  ..., -1.9922, -1.9844,  0.1875]]],\n",
       "        device='cuda:0'),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "55f295dd-9b59-43b8-b302-a23fd81692e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5d2754a1-9ab8-416f-804c-c164b27f1fbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(32, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change features\n",
    "IN_CHANNELS = 32\n",
    "NUM_CLASSES = 2\n",
    "device = 'cuda'\n",
    "\n",
    "accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "auroc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "model = torchvision.models.convnext_tiny(weights=convnext.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "model.features[0][0] = nn.Conv2d(IN_CHANNELS, 96, kernel_size=(4,4), stride=(4,4))\n",
    "model.classifier[2] = nn.Linear(768, NUM_CLASSES)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d6d0618f-948d-403b-9ef7-5be51c49f3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\600380055.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1388/1388 [00:47<00:00, 29.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 1.287624101500548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 347/347 [00:05<00:00, 64.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.6340, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.6340, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.6340, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1388/1388 [00:41<00:00, 33.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 1.1428112855701176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 347/347 [00:05<00:00, 63.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.6340, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.6340, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.6340, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1388/1388 [00:40<00:00, 34.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.655031361136725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 347/347 [00:05<00:00, 64.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: tensor(0.6340, device='cuda:0')\n",
      "Epoch 2: Validation Precision: tensor(0.6340, device='cuda:0')\n",
      "Epoch 2: Validation Recall: tensor(0.6340, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        # val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    # print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e0ccf-d920-4abf-99d8-44c4a11d5970",
   "metadata": {},
   "source": [
    "## Experiment: Classify dataset using all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ffab7611-26f9-4fb3-b7ef-c9bcc4af4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "LAYERS_INCLUDED = [1, 11, 21, 31]\n",
    "NUM_LAYER_LLM = len(LAYERS_INCLUDED)\n",
    "\n",
    "class AttentionMapBatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_df, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = annotations_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx = int(idx)\n",
    "        pt_path = self.root / self.files['filename'][f_idx]\n",
    "        f = torch.load(pt_path)\n",
    "        heads = f.flatten(0, 1)\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_label = torch.tensor(self.files['dataset'][f_idx] == 'science_elementary', dtype=torch.long)\n",
    "        # heads = heads.unsqueeze(dim=0)\n",
    "        # bucket_labels = bucket_label.unsqueeze(dim=0)\n",
    "        return heads.to(torch.float32), bucket_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "97c528f8-6115-4680-b12a-06119da69d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>prediction</th>\n",
       "      <th>correct</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1052</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\941_attent...</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>467</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\414_attent...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1660</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1174</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1368</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>1325</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1553</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>818</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\730_attent...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>159</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\137_attent...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>arc_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>1372</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>science_elementary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1735 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           filename prediction  \\\n",
       "0      1052  auxiliary_train\\arc_hard\\attentions\\941_attent...          D   \n",
       "1       467  auxiliary_train\\arc_hard\\attentions\\414_attent...          C   \n",
       "2      1660  auxiliary_train\\science_elementary\\attentions\\...          C   \n",
       "3      1174  auxiliary_train\\science_elementary\\attentions\\...          A   \n",
       "4      1368  auxiliary_train\\science_elementary\\attentions\\...          C   \n",
       "...     ...                                                ...        ...   \n",
       "1730   1325  auxiliary_train\\science_elementary\\attentions\\...          D   \n",
       "1731   1553  auxiliary_train\\science_elementary\\attentions\\...          A   \n",
       "1732    818  auxiliary_train\\arc_hard\\attentions\\730_attent...          A   \n",
       "1733    159  auxiliary_train\\arc_hard\\attentions\\137_attent...          B   \n",
       "1734   1372  auxiliary_train\\science_elementary\\attentions\\...          C   \n",
       "\n",
       "     correct             dataset  \n",
       "0          C            arc_hard  \n",
       "1          C            arc_hard  \n",
       "2          C  science_elementary  \n",
       "3          A  science_elementary  \n",
       "4          C  science_elementary  \n",
       "...      ...                 ...  \n",
       "1730       D  science_elementary  \n",
       "1731       A  science_elementary  \n",
       "1732       A            arc_hard  \n",
       "1733       B            arc_hard  \n",
       "1734       C  science_elementary  \n",
       "\n",
       "[1735 rows x 5 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(Path.home() / \"Downloads/mmlu_attention_files_list.txt\")\n",
    "dataset_df['dataset'] = dataset_df['filename'].apply(lambda filename: str(Path(filename).parent.parent.name))\n",
    "dataset_es = dataset_df[dataset_df['dataset'] == 'science_elementary'].reset_index()\n",
    "dataset_df = dataset_df.sample(frac=1).reset_index()\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9eb8975a-18d9-4c2d-a238-1c7d9dafe34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1384, 351)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "dataset = AttentionMapBatchDataset(Path.home() / \"Downloads/mmlu_output/\", dataset_df, transform=None)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "indices = torch.arange(len(dataset)).tolist()\n",
    "split_idx = int((len(indices)) * 0.8)\n",
    "train_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "test_data = torch.utils.data.Subset(dataset, indices[split_idx:])\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "39513149-79e7-47a3-8135-e9e099d0bd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "036ac084-4ecc-4ca1-a250-6f2d3ebd78e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\777388760.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.9531e-01, 3.0469e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [5.3125e-01, 4.1992e-01, 5.0293e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [6.7139e-03, 1.2398e-05, 7.8201e-05,  ..., 2.6562e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [4.9805e-02, 6.2943e-04, 5.3787e-04,  ..., 4.8047e-01,\n",
       "           8.0078e-02, 0.0000e+00],\n",
       "          [2.0294e-03, 5.0068e-06, 8.0466e-06,  ..., 7.6953e-01,\n",
       "           5.1025e-02, 1.7944e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [9.0625e-01, 9.5215e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.0078e-01, 1.7871e-01, 2.2461e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [4.8828e-02, 6.0425e-03, 4.6387e-03,  ..., 4.0527e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [1.5918e-01, 1.6724e-02, 6.3324e-04,  ..., 1.2891e-01,\n",
       "           1.9775e-02, 0.0000e+00],\n",
       "          [5.5664e-02, 8.6594e-04, 4.9591e-04,  ..., 2.3535e-01,\n",
       "           1.7285e-01, 1.1572e-01]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [3.7109e-01, 6.2891e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.8359e-02, 8.9844e-01, 3.2959e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [9.5215e-02, 4.5395e-04, 2.5749e-04,  ..., 6.4844e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [1.7090e-02, 3.8147e-04, 1.8358e-05,  ..., 9.1406e-01,\n",
       "           2.4536e-02, 0.0000e+00],\n",
       "          [3.1006e-02, 3.4904e-04, 6.4850e-05,  ..., 1.0791e-01,\n",
       "           4.0820e-01, 4.1211e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.0703e-01, 2.9297e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.9297e-01, 1.8359e-01, 2.4292e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [5.5078e-01, 2.0874e-02, 8.4839e-03,  ..., 9.7656e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [4.6094e-01, 1.4221e-02, 5.2795e-03,  ..., 1.2207e-01,\n",
       "           2.2827e-02, 0.0000e+00],\n",
       "          [5.0781e-01, 1.0315e-02, 4.8523e-03,  ..., 4.1504e-02,\n",
       "           9.3384e-03, 3.8086e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.6328e-01, 1.3477e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.7578e-01, 2.2461e-02, 3.0078e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [5.3125e-01, 7.2937e-03, 1.3965e-01,  ..., 1.5320e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [4.9219e-01, 2.2430e-03, 5.8105e-02,  ..., 3.1982e-02,\n",
       "           4.4189e-02, 0.0000e+00],\n",
       "          [4.3164e-01, 2.5635e-03, 1.0596e-01,  ..., 9.5825e-03,\n",
       "           2.7710e-02, 1.5076e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.7930e-01, 7.1875e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.7500e-01, 4.8340e-02, 7.5195e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [7.5781e-01, 1.1658e-02, 1.1475e-02,  ..., 6.6895e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.5000e-01, 6.8054e-03, 4.9744e-03,  ..., 2.7710e-02,\n",
       "           6.0547e-02, 0.0000e+00],\n",
       "          [6.8750e-01, 7.7209e-03, 5.7373e-03,  ..., 3.0762e-02,\n",
       "           6.6406e-02, 5.0781e-02]]], device='cuda:0'),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[split_idx-42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1166edd8-fef5-4d6d-94dc-b15e20d7b086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\777388760.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.9531e-01, 3.0469e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [5.3125e-01, 4.1992e-01, 5.0293e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [2.5940e-03, 2.5511e-05, 3.8719e-04,  ..., 1.6895e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.8687e-02, 3.1090e-04, 2.0294e-03,  ..., 3.9258e-01,\n",
       "           8.6426e-02, 0.0000e+00],\n",
       "          [7.2861e-04, 2.6822e-07, 2.6584e-05,  ..., 7.1484e-01,\n",
       "           5.8105e-02, 2.0630e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [9.0625e-01, 9.5215e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.0078e-01, 1.7871e-01, 2.2461e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [4.7119e-02, 3.6926e-03, 4.4250e-03,  ..., 5.5908e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [1.5723e-01, 1.0193e-02, 5.6458e-04,  ..., 1.2305e-01,\n",
       "           2.0630e-02, 0.0000e+00],\n",
       "          [5.6641e-02, 3.5477e-04, 2.3651e-04,  ..., 1.7676e-01,\n",
       "           1.6504e-01, 1.0986e-01]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [3.7109e-01, 6.2891e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.8359e-02, 8.9844e-01, 3.2959e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [1.6968e-02, 3.6210e-06, 3.2616e-04,  ..., 6.1719e-01,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.4697e-03, 8.5682e-07, 7.5623e-07,  ..., 9.2188e-01,\n",
       "           2.6245e-02, 0.0000e+00],\n",
       "          [1.6113e-02, 1.0654e-06, 8.4043e-06,  ..., 9.3750e-02,\n",
       "           4.1992e-01, 4.2969e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.0703e-01, 2.9297e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.9297e-01, 1.8359e-01, 2.4292e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [4.2188e-01, 1.9165e-02, 5.7373e-03,  ..., 9.0332e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [4.2383e-01, 1.2695e-02, 4.4861e-03,  ..., 1.4844e-01,\n",
       "           1.7212e-02, 0.0000e+00],\n",
       "          [4.4531e-01, 7.0496e-03, 3.9062e-03,  ..., 4.6631e-02,\n",
       "           7.9346e-03, 2.7344e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.6328e-01, 1.3477e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [6.7578e-01, 2.2461e-02, 3.0078e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [2.9102e-01, 1.1475e-02, 1.4355e-01,  ..., 1.5381e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [4.9805e-01, 1.8539e-03, 4.3457e-02,  ..., 3.7354e-02,\n",
       "           4.6631e-02, 0.0000e+00],\n",
       "          [3.3594e-01, 1.6861e-03, 8.6914e-02,  ..., 9.4604e-03,\n",
       "           2.5757e-02, 1.0071e-02]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.7930e-01, 7.1875e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.7500e-01, 4.8340e-02, 7.5195e-02,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [7.5000e-01, 1.0437e-02, 1.0132e-02,  ..., 4.8340e-02,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.2266e-01, 7.7820e-03, 5.0049e-03,  ..., 2.8442e-02,\n",
       "           4.3945e-02, 0.0000e+00],\n",
       "          [6.6016e-01, 8.6060e-03, 5.4932e-03,  ..., 3.4912e-02,\n",
       "           4.6875e-02, 3.8574e-02]]], device='cuda:0'),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "080b6978-e277-41dd-8d76-1d77994c93f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\777388760.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [6.9531e-01, 3.0469e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [5.3125e-01, 4.1992e-01, 5.0293e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [1.5747e-02, 2.0409e-04, 4.9210e-04,  ..., 2.9102e-01,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.0312e-02, 8.0872e-04, 5.9128e-04,  ..., 4.9609e-01,\n",
       "          8.2520e-02, 0.0000e+00],\n",
       "         [5.7373e-03, 2.8968e-05, 4.7386e-06,  ..., 7.9688e-01,\n",
       "          5.0537e-02, 1.8188e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [9.0625e-01, 9.5215e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [8.0078e-01, 1.7676e-01, 2.2461e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [7.3242e-02, 1.7700e-02, 4.6082e-03,  ..., 3.5400e-02,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.9336e-01, 3.3203e-02, 5.4169e-04,  ..., 1.2402e-01,\n",
       "          1.9043e-02, 0.0000e+00],\n",
       "         [7.0312e-02, 5.8289e-03, 4.9210e-04,  ..., 2.2949e-01,\n",
       "          1.6602e-01, 1.1182e-01]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7109e-01, 6.2891e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [6.8359e-02, 8.9844e-01, 3.2471e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [8.4473e-02, 2.1362e-04, 1.1396e-04,  ..., 7.1094e-01,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.8677e-02, 1.8692e-04, 5.2527e-07,  ..., 9.2188e-01,\n",
       "          2.5024e-02, 0.0000e+00],\n",
       "         [4.2969e-02, 5.4932e-04, 8.1062e-06,  ..., 1.0498e-01,\n",
       "          4.0234e-01, 4.1016e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.1484e-01, 2.8711e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.9688e-01, 1.7969e-01, 2.3926e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [4.1992e-01, 1.9775e-02, 1.0010e-02,  ..., 7.6660e-02,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [4.1211e-01, 1.4465e-02, 4.7913e-03,  ..., 1.4746e-01,\n",
       "          1.4893e-02, 0.0000e+00],\n",
       "         [4.8242e-01, 6.3782e-03, 5.5237e-03,  ..., 4.0527e-02,\n",
       "          8.7891e-03, 3.4180e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [8.6719e-01, 1.3184e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [6.8750e-01, 2.2949e-02, 2.9102e-01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [3.1836e-01, 8.6670e-03, 2.4316e-01,  ..., 1.8677e-02,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [5.2734e-01, 2.0294e-03, 6.3477e-02,  ..., 3.4668e-02,\n",
       "          5.2734e-02, 0.0000e+00],\n",
       "         [4.1406e-01, 1.4725e-03, 1.1426e-01,  ..., 8.9722e-03,\n",
       "          2.2217e-02, 1.4954e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.7930e-01, 7.1875e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [8.7500e-01, 4.8584e-02, 7.4707e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [7.3438e-01, 1.2512e-02, 1.0864e-02,  ..., 6.0303e-02,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.2656e-01, 5.4626e-03, 3.8147e-03,  ..., 3.7109e-02,\n",
       "          5.5420e-02, 0.0000e+00],\n",
       "         [6.7188e-01, 9.8267e-03, 5.3406e-03,  ..., 3.7109e-02,\n",
       "          5.5664e-02, 5.0781e-02]]], device='cuda:0')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4aea452a-0d8b-4634-8644-2cd364061180",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "846b448e-cf64-4fb6-a0db-1e5c923b10a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(1024, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change features\n",
    "IN_CHANNELS = 1024\n",
    "NUM_CLASSES = 2\n",
    "device = 'cuda'\n",
    "\n",
    "accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "auroc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "model = torchvision.models.convnext_tiny()\n",
    "model.features[0][0] = nn.Conv2d(IN_CHANNELS, 96, kernel_size=(4,4), stride=(4,4))\n",
    "model.classifier[2] = nn.Linear(768, NUM_CLASSES)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "73412b5b-6859-456d-810c-8a131d806b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_22100\\777388760.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:42<00:00, 32.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 1.1579424295978606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:05<00:00, 62.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.3276, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.3276, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.3276, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:43<00:00, 31.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 1.6788336689970602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:05<00:00, 64.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.3276, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.3276, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.3276, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:40<00:00, 34.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.811341903745736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:05<00:00, 64.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: tensor(0.5954, device='cuda:0')\n",
      "Epoch 2: Validation Precision: tensor(0.5954, device='cuda:0')\n",
      "Epoch 2: Validation Recall: tensor(0.5954, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:41<00:00, 33.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.6798322808584252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:05<00:00, 64.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Accuracy: tensor(0.5954, device='cuda:0')\n",
      "Epoch 3: Validation Precision: tensor(0.5954, device='cuda:0')\n",
      "Epoch 3: Validation Recall: tensor(0.5954, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:43<00:00, 31.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.6448912463134768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:05<00:00, 62.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Accuracy: tensor(0.5954, device='cuda:0')\n",
      "Epoch 4: Validation Precision: tensor(0.5954, device='cuda:0')\n",
      "Epoch 4: Validation Recall: tensor(0.5954, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:40<00:00, 34.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.6464135715619505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:05<00:00, 64.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Accuracy: tensor(0.5954, device='cuda:0')\n",
      "Epoch 5: Validation Precision: tensor(0.5954, device='cuda:0')\n",
      "Epoch 5: Validation Recall: tensor(0.5954, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.25)\n",
    "\n",
    "num_epochs = 6\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        # val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    # print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408db62-aae9-4074-8465-0d1adf3cfe18",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
