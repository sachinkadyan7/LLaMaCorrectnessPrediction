{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8fb0b4-a0fd-4489-8b07-acda5f4e0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision.models import convnext\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2001f3-8a56-410d-a76b-3f141a2876d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.convnext_tiny(weights=convnext.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90b045-c306-4aa1-b85e-94f60ed57587",
   "metadata": {},
   "source": [
    "## Experiment: Classify a single layer through all its 32 heads' attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62571827-054e-439c-a3ac-b5afcf741ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "LAYERS_INCLUDED = [1, 11, 21, 31]\n",
    "NUM_LAYER_LLM = len(LAYERS_INCLUDED)\n",
    "\n",
    "class AttentionMapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_df, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = annotations_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files) * NUM_LAYER_LLM\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx = int(idx // NUM_LAYER_LLM)\n",
    "        pt_path = self.root / self.files['filename'][f_idx]\n",
    "        f = torch.load(pt_path)\n",
    "        random_layer = idx % NUM_LAYER_LLM\n",
    "        layer_idx = LAYERS_INCLUDED[random_layer]\n",
    "        heads = f[layer_idx]\n",
    "        shape = heads.shape\n",
    "        uti = torch.triu_indices(shape[-2], shape[-1])\n",
    "        lti = torch.tril_indices(shape[-2], shape[-1])\n",
    "        for h in range(len(heads)):\n",
    "            heads[h][uti[0], uti[1]] = heads[h][uti[1], uti[0]]\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_label = torch.tensor(random_layer)\n",
    "        heads.unsqueeze(dim=0)\n",
    "        bucket_label.unsqueeze(dim=0)\n",
    "        return heads.to(torch.float32), bucket_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de80cd79-0e9e-47f3-b0dd-b817888e9bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>prediction</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1391</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1507</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>986</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\882_attent...</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\1094_atten...</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1533</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>1071</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\959_attent...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1652</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>1253</td>\n",
       "      <td>auxiliary_train\\science_elementary\\attentions\\...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>486</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\431_attent...</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>140</td>\n",
       "      <td>auxiliary_train\\arc_hard\\attentions\\11_attenti...</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1735 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           filename prediction  \\\n",
       "0      1391  auxiliary_train\\science_elementary\\attentions\\...          D   \n",
       "1      1507  auxiliary_train\\science_elementary\\attentions\\...          B   \n",
       "2       986  auxiliary_train\\arc_hard\\attentions\\882_attent...          C   \n",
       "3       104  auxiliary_train\\arc_hard\\attentions\\1094_atten...          C   \n",
       "4      1533  auxiliary_train\\science_elementary\\attentions\\...          C   \n",
       "...     ...                                                ...        ...   \n",
       "1730   1071  auxiliary_train\\arc_hard\\attentions\\959_attent...          B   \n",
       "1731   1652  auxiliary_train\\science_elementary\\attentions\\...          B   \n",
       "1732   1253  auxiliary_train\\science_elementary\\attentions\\...          C   \n",
       "1733    486  auxiliary_train\\arc_hard\\attentions\\431_attent...          D   \n",
       "1734    140  auxiliary_train\\arc_hard\\attentions\\11_attenti...          A   \n",
       "\n",
       "     correct  \n",
       "0          D  \n",
       "1          B  \n",
       "2          D  \n",
       "3          D  \n",
       "4          C  \n",
       "...      ...  \n",
       "1730       B  \n",
       "1731       B  \n",
       "1732       C  \n",
       "1733       C  \n",
       "1734       B  \n",
       "\n",
       "[1735 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(Path.home() / \"Downloads/mmlu_attention_files_list.txt\")\n",
    "dataset_df = dataset_df.sample(frac=1).reset_index()\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2e40c61-cb11-4662-8867-f995531b21c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5552, 1388)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(0.5),\n",
    "    # transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "dataset = AttentionMapDataset(Path.home() / \"Downloads/mmlu_output/\", dataset_df, transform=None)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "indices = torch.arange(len(dataset)).tolist()\n",
    "split_idx = int((len(indices) // NUM_LAYER_LLM) * 0.8) * NUM_LAYER_LLM\n",
    "train_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "test_data = torch.utils.data.Subset(dataset, indices[split_idx:])\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a31d705-f1b8-4ae7-b1c1-ed0e507d10ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5552"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "091aa12d-e065-4447-8f1b-230bc6a1c52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_23620\\2845798312.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000e+00, 6.5234e-01, 5.4688e-01,  ..., 3.7305e-01,\n",
       "           2.2852e-01, 2.8125e-01],\n",
       "          [6.5234e-01, 3.4570e-01, 5.8838e-02,  ..., 8.2397e-03,\n",
       "           3.7575e-04, 4.4441e-04],\n",
       "          [5.4688e-01, 5.8838e-02, 3.9258e-01,  ..., 4.5471e-03,\n",
       "           1.2741e-03, 8.8501e-04],\n",
       "          ...,\n",
       "          [3.7305e-01, 8.2397e-03, 4.5471e-03,  ..., 3.7891e-01,\n",
       "           1.1865e-01, 3.7354e-02],\n",
       "          [2.2852e-01, 3.7575e-04, 1.2741e-03,  ..., 1.1865e-01,\n",
       "           5.6641e-01, 8.2031e-02],\n",
       "          [2.8125e-01, 4.4441e-04, 8.8501e-04,  ..., 3.7354e-02,\n",
       "           8.2031e-02, 4.8047e-01]],\n",
       " \n",
       "         [[1.0000e+00, 7.1094e-01, 8.7109e-01,  ..., 6.1328e-01,\n",
       "           8.0469e-01, 6.0938e-01],\n",
       "          [7.1094e-01, 2.8906e-01, 4.2236e-02,  ..., 4.0588e-03,\n",
       "           4.6349e-04, 1.8406e-04],\n",
       "          [8.7109e-01, 4.2236e-02, 8.7402e-02,  ..., 2.1820e-03,\n",
       "           1.8921e-03, 3.5477e-04],\n",
       "          ...,\n",
       "          [6.1328e-01, 4.0588e-03, 2.1820e-03,  ..., 1.8652e-01,\n",
       "           6.1279e-02, 2.0630e-02],\n",
       "          [8.0469e-01, 4.6349e-04, 1.8921e-03,  ..., 6.1279e-02,\n",
       "           8.3984e-02, 4.8096e-02],\n",
       "          [6.0938e-01, 1.8406e-04, 3.5477e-04,  ..., 2.0630e-02,\n",
       "           4.8096e-02, 2.8320e-01]],\n",
       " \n",
       "         [[1.0000e+00, 2.5977e-01, 6.5625e-01,  ..., 3.9258e-01,\n",
       "           6.0938e-01, 5.7031e-01],\n",
       "          [2.5977e-01, 7.3828e-01, 3.0640e-02,  ..., 3.1891e-03,\n",
       "           9.3842e-04, 5.7220e-04],\n",
       "          [6.5625e-01, 3.0640e-02, 3.1250e-01,  ..., 3.0823e-03,\n",
       "           7.5150e-04, 6.0654e-04],\n",
       "          ...,\n",
       "          [3.9258e-01, 3.1891e-03, 3.0823e-03,  ..., 4.7461e-01,\n",
       "           6.2500e-02, 1.9165e-02],\n",
       "          [6.0938e-01, 9.3842e-04, 7.5150e-04,  ..., 6.2500e-02,\n",
       "           2.5781e-01, 6.1523e-02],\n",
       "          [5.7031e-01, 5.7220e-04, 6.0654e-04,  ..., 1.9165e-02,\n",
       "           6.1523e-02, 2.9297e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.0000e+00, 7.0703e-01, 7.9297e-01,  ..., 4.8438e-01,\n",
       "           4.0625e-01, 4.9805e-01],\n",
       "          [7.0703e-01, 2.9297e-01, 1.8359e-01,  ..., 2.3804e-02,\n",
       "           1.3245e-02, 1.1597e-02],\n",
       "          [7.9297e-01, 1.8359e-01, 2.4292e-02,  ..., 9.1553e-03,\n",
       "           5.7068e-03, 5.5237e-03],\n",
       "          ...,\n",
       "          [4.8438e-01, 2.3804e-02, 9.1553e-03,  ..., 9.3262e-02,\n",
       "           1.6211e-01, 4.2725e-02],\n",
       "          [4.0625e-01, 1.3245e-02, 5.7068e-03,  ..., 1.6211e-01,\n",
       "           2.1484e-02, 8.6060e-03],\n",
       "          [4.9805e-01, 1.1597e-02, 5.5237e-03,  ..., 4.2725e-02,\n",
       "           8.6060e-03, 4.0527e-02]],\n",
       " \n",
       "         [[1.0000e+00, 8.6328e-01, 6.7578e-01,  ..., 4.0039e-01,\n",
       "           3.9258e-01, 3.6523e-01],\n",
       "          [8.6328e-01, 1.3477e-01, 2.2461e-02,  ..., 6.5308e-03,\n",
       "           1.4877e-03, 1.9455e-03],\n",
       "          [6.7578e-01, 2.2461e-02, 3.0078e-01,  ..., 1.7383e-01,\n",
       "           5.2979e-02, 1.0156e-01],\n",
       "          ...,\n",
       "          [4.0039e-01, 6.5308e-03, 1.7383e-01,  ..., 1.9409e-02,\n",
       "           4.3701e-02, 8.4839e-03],\n",
       "          [3.9258e-01, 1.4877e-03, 5.2979e-02,  ..., 4.3701e-02,\n",
       "           7.1289e-02, 2.9419e-02],\n",
       "          [3.6523e-01, 1.9455e-03, 1.0156e-01,  ..., 8.4839e-03,\n",
       "           2.9419e-02, 1.7334e-02]],\n",
       " \n",
       "         [[1.0000e+00, 2.7930e-01, 8.7500e-01,  ..., 7.1484e-01,\n",
       "           6.9922e-01, 6.6406e-01],\n",
       "          [2.7930e-01, 7.1875e-01, 4.8340e-02,  ..., 1.6479e-02,\n",
       "           4.9744e-03, 8.7280e-03],\n",
       "          [8.7500e-01, 4.8340e-02, 7.5195e-02,  ..., 1.2634e-02,\n",
       "           3.8605e-03, 4.3945e-03],\n",
       "          ...,\n",
       "          [7.1484e-01, 1.6479e-02, 1.2634e-02,  ..., 7.8613e-02,\n",
       "           3.1982e-02, 3.6865e-02],\n",
       "          [6.9922e-01, 4.9744e-03, 3.8605e-03,  ..., 3.1982e-02,\n",
       "           7.4219e-02, 7.4219e-02],\n",
       "          [6.6406e-01, 8.7280e-03, 4.3945e-03,  ..., 3.6865e-02,\n",
       "           7.4219e-02, 5.3955e-02]]], device='cuda:0'),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[split_idx+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad8b5043-3d2c-4aa5-bec5-b36d38dc5f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_23620\\2845798312.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000e+00, 9.7266e-01, 7.8906e-01,  ..., 7.5781e-01,\n",
       "           8.4766e-01, 8.6328e-01],\n",
       "          [9.7266e-01, 2.6367e-02, 1.8457e-01,  ..., 3.5858e-04,\n",
       "           1.5736e-04, 4.3631e-05],\n",
       "          [7.8906e-01, 1.8457e-01, 2.5391e-02,  ..., 1.0920e-04,\n",
       "           1.2457e-05, 1.0371e-05],\n",
       "          ...,\n",
       "          [7.5781e-01, 3.5858e-04, 1.0920e-04,  ..., 4.9316e-02,\n",
       "           9.0332e-02, 3.0151e-02],\n",
       "          [8.4766e-01, 1.5736e-04, 1.2457e-05,  ..., 9.0332e-02,\n",
       "           2.4780e-02, 6.8359e-02],\n",
       "          [8.6328e-01, 4.3631e-05, 1.0371e-05,  ..., 3.0151e-02,\n",
       "           6.8359e-02, 2.2095e-02]],\n",
       " \n",
       "         [[1.0000e+00, 9.8438e-01, 8.5938e-01,  ..., 9.2969e-01,\n",
       "           8.6719e-01, 9.4922e-01],\n",
       "          [9.8438e-01, 1.3977e-02, 8.9844e-02,  ..., 6.5994e-04,\n",
       "           9.7752e-05, 7.2002e-05],\n",
       "          [8.5938e-01, 8.9844e-02, 4.9072e-02,  ..., 2.7466e-04,\n",
       "           4.6015e-05, 3.0994e-05],\n",
       "          ...,\n",
       "          [9.2969e-01, 6.5994e-04, 2.7466e-04,  ..., 2.0508e-02,\n",
       "           6.4453e-02, 1.1597e-02],\n",
       "          [8.6719e-01, 9.7752e-05, 4.6015e-05,  ..., 6.4453e-02,\n",
       "           3.7109e-02, 1.4893e-02],\n",
       "          [9.4922e-01, 7.2002e-05, 3.0994e-05,  ..., 1.1597e-02,\n",
       "           1.4893e-02, 1.2695e-02]],\n",
       " \n",
       "         [[1.0000e+00, 9.9219e-01, 6.7578e-01,  ..., 3.7891e-01,\n",
       "           4.5898e-01, 4.4141e-01],\n",
       "          [9.9219e-01, 9.1553e-03, 2.6172e-01,  ..., 2.2266e-01,\n",
       "           1.4062e-01, 1.5527e-01],\n",
       "          [6.7578e-01, 2.6172e-01, 6.3477e-02,  ..., 9.8877e-03,\n",
       "           3.4790e-03, 5.1270e-03],\n",
       "          ...,\n",
       "          [3.7891e-01, 2.2266e-01, 9.8877e-03,  ..., 5.8838e-02,\n",
       "           6.6895e-02, 5.0781e-02],\n",
       "          [4.5898e-01, 1.4062e-01, 3.4790e-03,  ..., 6.6895e-02,\n",
       "           4.8340e-02, 2.6733e-02],\n",
       "          [4.4141e-01, 1.5527e-01, 5.1270e-03,  ..., 5.0781e-02,\n",
       "           2.6733e-02, 2.5269e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.0000e+00, 9.7656e-01, 8.7891e-01,  ..., 6.5234e-01,\n",
       "           6.3281e-01, 6.9531e-01],\n",
       "          [9.7656e-01, 2.1606e-02, 1.0889e-01,  ..., 9.0942e-03,\n",
       "           1.7944e-02, 7.6599e-03],\n",
       "          [8.7891e-01, 1.0889e-01, 1.2329e-02,  ..., 5.2490e-03,\n",
       "           3.2196e-03, 2.2888e-03],\n",
       "          ...,\n",
       "          [6.5234e-01, 9.0942e-03, 5.2490e-03,  ..., 3.0029e-02,\n",
       "           3.7109e-02, 1.8188e-02],\n",
       "          [6.3281e-01, 1.7944e-02, 3.2196e-03,  ..., 3.7109e-02,\n",
       "           4.2725e-02, 4.5654e-02],\n",
       "          [6.9531e-01, 7.6599e-03, 2.2888e-03,  ..., 1.8188e-02,\n",
       "           4.5654e-02, 2.5757e-02]],\n",
       " \n",
       "         [[1.0000e+00, 9.7656e-01, 9.6875e-01,  ..., 8.0078e-01,\n",
       "           8.3594e-01, 8.4766e-01],\n",
       "          [9.7656e-01, 2.1606e-02, 7.1716e-03,  ..., 5.6076e-04,\n",
       "           9.9182e-04, 4.9210e-04],\n",
       "          [9.6875e-01, 7.1716e-03, 2.3926e-02,  ..., 2.1267e-04,\n",
       "           6.8283e-04, 9.3937e-05],\n",
       "          ...,\n",
       "          [8.0078e-01, 5.6076e-04, 2.1267e-04,  ..., 2.0020e-02,\n",
       "           1.3000e-02, 8.8501e-03],\n",
       "          [8.3594e-01, 9.9182e-04, 6.8283e-04,  ..., 1.3000e-02,\n",
       "           3.5156e-02, 7.3730e-02],\n",
       "          [8.4766e-01, 4.9210e-04, 9.3937e-05,  ..., 8.8501e-03,\n",
       "           7.3730e-02, 1.9653e-02]],\n",
       " \n",
       "         [[1.0000e+00, 9.2188e-01, 9.2969e-01,  ..., 5.1172e-01,\n",
       "           5.9766e-01, 6.3281e-01],\n",
       "          [9.2188e-01, 7.7148e-02, 3.2959e-02,  ..., 2.0020e-02,\n",
       "           7.8735e-03, 6.9275e-03],\n",
       "          [9.2969e-01, 3.2959e-02, 3.9307e-02,  ..., 2.6703e-03,\n",
       "           1.9531e-03, 1.7471e-03],\n",
       "          ...,\n",
       "          [5.1172e-01, 2.0020e-02, 2.6703e-03,  ..., 7.5195e-02,\n",
       "           4.2725e-02, 3.4180e-02],\n",
       "          [5.9766e-01, 7.8735e-03, 1.9531e-03,  ..., 4.2725e-02,\n",
       "           6.1768e-02, 3.4912e-02],\n",
       "          [6.3281e-01, 6.9275e-03, 1.7471e-03,  ..., 3.4180e-02,\n",
       "           3.4912e-02, 5.1270e-02]]], device='cuda:0'),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3374f21f-7889-4b6c-992f-199a74864dd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_23620\\2845798312.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000e+00, 6.5625e-01, 5.5469e-01,  ..., 4.1406e-01,\n",
       "           2.3633e-01, 2.3535e-01],\n",
       "          [6.5625e-01, 3.4180e-01, 5.4199e-02,  ..., 6.6528e-03,\n",
       "           3.2997e-04, 4.4823e-04],\n",
       "          [5.5469e-01, 5.4199e-02, 3.9062e-01,  ..., 2.8992e-03,\n",
       "           3.4332e-03, 1.7166e-03],\n",
       "          ...,\n",
       "          [4.1406e-01, 6.6528e-03, 2.8992e-03,  ..., 3.1055e-01,\n",
       "           1.1523e-01, 4.4434e-02],\n",
       "          [2.3633e-01, 3.2997e-04, 3.4332e-03,  ..., 1.1523e-01,\n",
       "           5.6250e-01, 6.9336e-02],\n",
       "          [2.3535e-01, 4.4823e-04, 1.7166e-03,  ..., 4.4434e-02,\n",
       "           6.9336e-02, 5.1953e-01]],\n",
       " \n",
       "         [[1.0000e+00, 7.1484e-01, 8.7500e-01,  ..., 5.2734e-01,\n",
       "           7.9297e-01, 6.2500e-01],\n",
       "          [7.1484e-01, 2.8516e-01, 3.9551e-02,  ..., 2.8992e-03,\n",
       "           1.0071e-03, 3.9482e-04],\n",
       "          [8.7500e-01, 3.9551e-02, 8.4961e-02,  ..., 6.9046e-04,\n",
       "           8.6212e-04, 7.1335e-04],\n",
       "          ...,\n",
       "          [5.2734e-01, 2.8992e-03, 6.9046e-04,  ..., 2.2266e-01,\n",
       "           7.4707e-02, 2.6367e-02],\n",
       "          [7.9297e-01, 1.0071e-03, 8.6212e-04,  ..., 7.4707e-02,\n",
       "           8.6426e-02, 4.6631e-02],\n",
       "          [6.2500e-01, 3.9482e-04, 7.1335e-04,  ..., 2.6367e-02,\n",
       "           4.6631e-02, 2.4902e-01]],\n",
       " \n",
       "         [[1.0000e+00, 2.5781e-01, 6.5625e-01,  ..., 3.2227e-01,\n",
       "           6.2500e-01, 6.0547e-01],\n",
       "          [2.5781e-01, 7.4219e-01, 2.9297e-02,  ..., 2.9907e-03,\n",
       "           7.3242e-04, 2.9564e-04],\n",
       "          [6.5625e-01, 2.9297e-02, 3.1445e-01,  ..., 2.4109e-03,\n",
       "           2.1210e-03, 1.2131e-03],\n",
       "          ...,\n",
       "          [3.2227e-01, 2.9907e-03, 2.4109e-03,  ..., 5.0000e-01,\n",
       "           7.6172e-02, 2.9297e-02],\n",
       "          [6.2500e-01, 7.3242e-04, 2.1210e-03,  ..., 7.6172e-02,\n",
       "           2.2070e-01, 4.2725e-02],\n",
       "          [6.0547e-01, 2.9564e-04, 1.2131e-03,  ..., 2.9297e-02,\n",
       "           4.2725e-02, 2.4023e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.0000e+00, 7.1484e-01, 8.0078e-01,  ..., 4.0820e-01,\n",
       "           4.3945e-01, 4.6094e-01],\n",
       "          [7.1484e-01, 2.8711e-01, 1.7773e-01,  ..., 1.3855e-02,\n",
       "           1.4771e-02, 8.3618e-03],\n",
       "          [8.0078e-01, 1.7773e-01, 2.3438e-02,  ..., 1.1047e-02,\n",
       "           4.2419e-03, 4.7913e-03],\n",
       "          ...,\n",
       "          [4.0820e-01, 1.3855e-02, 1.1047e-02,  ..., 7.3730e-02,\n",
       "           1.3672e-01, 4.0527e-02],\n",
       "          [4.3945e-01, 1.4771e-02, 4.2419e-03,  ..., 1.3672e-01,\n",
       "           1.5381e-02, 8.6060e-03],\n",
       "          [4.6094e-01, 8.3618e-03, 4.7913e-03,  ..., 4.0527e-02,\n",
       "           8.6060e-03, 3.9062e-02]],\n",
       " \n",
       "         [[1.0000e+00, 8.7109e-01, 6.8359e-01,  ..., 3.3594e-01,\n",
       "           5.2734e-01, 4.0820e-01],\n",
       "          [8.7109e-01, 1.2891e-01, 2.3071e-02,  ..., 4.8828e-03,\n",
       "           1.7014e-03, 1.8082e-03],\n",
       "          [6.8359e-01, 2.3071e-02, 2.9297e-01,  ..., 2.0898e-01,\n",
       "           5.3223e-02, 9.7168e-02],\n",
       "          ...,\n",
       "          [3.3594e-01, 4.8828e-03, 2.0898e-01,  ..., 1.8677e-02,\n",
       "           3.6621e-02, 9.6436e-03],\n",
       "          [5.2734e-01, 1.7014e-03, 5.3223e-02,  ..., 3.6621e-02,\n",
       "           5.2490e-02, 2.4536e-02],\n",
       "          [4.0820e-01, 1.8082e-03, 9.7168e-02,  ..., 9.6436e-03,\n",
       "           2.4536e-02, 1.5015e-02]],\n",
       " \n",
       "         [[1.0000e+00, 2.7734e-01, 8.7500e-01,  ..., 7.1875e-01,\n",
       "           7.4219e-01, 6.7188e-01],\n",
       "          [2.7734e-01, 7.2266e-01, 4.9805e-02,  ..., 1.2390e-02,\n",
       "           7.0496e-03, 1.0376e-02],\n",
       "          [8.7500e-01, 4.9805e-02, 7.4219e-02,  ..., 9.2163e-03,\n",
       "           5.0049e-03, 5.1270e-03],\n",
       "          ...,\n",
       "          [7.1875e-01, 1.2390e-02, 9.2163e-03,  ..., 6.2500e-02,\n",
       "           2.8809e-02, 3.4424e-02],\n",
       "          [7.4219e-01, 7.0496e-03, 5.0049e-03,  ..., 2.8809e-02,\n",
       "           5.0781e-02, 5.8105e-02],\n",
       "          [6.7188e-01, 1.0376e-02, 5.1270e-03,  ..., 3.4424e-02,\n",
       "           5.8105e-02, 4.5410e-02]]], device='cuda:0'),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91f0e4ed-1a3b-47e9-b393-60f3923da47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33cc6836-2872-45ec-821d-2262745170bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(32, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change features\n",
    "IN_CHANNELS = 32\n",
    "NUM_CLASSES = NUM_LAYER_LLM\n",
    "device = 'cuda'\n",
    "\n",
    "accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "auroc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "model.features[0][0] = nn.Conv2d(IN_CHANNELS, 96, kernel_size=(4,4), stride=(4,4))\n",
    "model.classifier[2] = nn.Linear(768, NUM_CLASSES)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98980bd8-118b-44b8-81b4-d496e10da442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_23620\\2845798312.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 5552/5552 [03:12<00:00, 28.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 18.265812699959792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|██████████████████████████████████████████████████████████████████| 1388/1388 [00:30<00:00, 45.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.2500, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.2500, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.2500, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 5552/5552 [03:19<00:00, 27.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 1.390085073858929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|██████████████████████████████████████████████████████████████████| 1388/1388 [00:31<00:00, 44.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.2500, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.2500, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.2500, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining:   1%|▍                                                                     | 33/5552 [00:01<03:21, 27.39it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m tr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m nb_tr_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining: \u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[53], line 18\u001b[0m, in \u001b[0;36mAttentionMapDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     16\u001b[0m f_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m NUM_LAYER_LLM)\n\u001b[0;32m     17\u001b[0m pt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m][f_idx]\n\u001b[1;32m---> 18\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m random_layer \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m%\u001b[39m NUM_LAYER_LLM\n\u001b[0;32m     20\u001b[0m layer_idx \u001b[38;5;241m=\u001b[39m LAYERS_INCLUDED[random_layer]\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1361\u001b[0m             opened_zipfile,\n\u001b[0;32m   1362\u001b[0m             map_location,\n\u001b[0;32m   1363\u001b[0m             pickle_module,\n\u001b[0;32m   1364\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1365\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1366\u001b[0m         )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\serialization.py:643\u001b[0m, in \u001b[0;36m_open_file.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        # val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    # print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28b88c-7c29-46a0-8b62-1429047a8ee5",
   "metadata": {},
   "source": [
    "## Experiment: Classify correctness by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436b9ee-4676-44b1-8ea2-0ab2159d7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "LAYERS_INCLUDED = [1, 11, 21, 31]\n",
    "NUM_LAYER_LLM = len(LAYERS_INCLUDED)\n",
    "\n",
    "class AttentionMapDFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_file, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files) * NUM_LAYER_LLM\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx = int(idx // NUM_LAYER_LLM)\n",
    "        pt_path = self.root / self.files['filename'][f_idx]\n",
    "        f = torch.load(pt_path)\n",
    "        random_layer = idx % NUM_LAYER_LLM\n",
    "        layer_idx = LAYERS_INCLUDED[random_layer]\n",
    "        heads = f[layer_idx]\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_label = torch.tensor(random_layer)\n",
    "        heads.unsqueeze(dim=0)\n",
    "        bucket_label.unsqueeze(dim=0)\n",
    "        return heads.to(torch.float32), bucket_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664d238-0a8b-4733-ac31-7d645e8eb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(Path.home() / \"Downloads/mmlu_attention_files_list.txt\")\n",
    "dataset_df['dataset'] = dataset_df['filename'].apply(lambda filename: str(Path(filename).parent.parent.name))\n",
    "dataset_es = dataset_df[dataset_df['dataset'] == 'elementary_science']\n",
    "\n",
    "dataset_es = AttentionMapDFDataset(Path.home / \"Downloads/mmlu_output/\", dataset_es, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e0ccf-d920-4abf-99d8-44c4a11d5970",
   "metadata": {},
   "source": [
    "## Experiment: Classify correctness by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2615e9ab-3129-4306-bab7-3de44088beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionMapBatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_file, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pt_path = self.root / self.files['filename'][idx]\n",
    "        f = torch.load(pt_path)\n",
    "        heads = torch.stack(f).squeeze()\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_labels = torch.arange(NUM_LAYER_LLM)\n",
    "        return heads.to(torch.float32), bucket_labels.to(torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11276af7-97b2-4508-a4ee-3951d2ffec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "dataset = AttentionMapBatchDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_list.txt\", transform=transform)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "split_idx = int(len(indices) * 0.8)\n",
    "training_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "test_data = torch.utils.data.Subset(dataset, indices[split_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ea085b-520b-4b9b-afbc-ea5f65eb4461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1068, 267)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0c41c4-d2ff-4b49-999d-7e32bd40a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "824676ff-57fc-4a0c-bc53-613cff267b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\560115562.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6250, -1.6250, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.9062, -1.9688,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.7500, -1.8594, -1.7578,  ..., -1.7266, -2.0000, -2.0000],\n",
       "           [ 0.1562, -1.9766, -1.9922,  ..., -1.8984, -1.9609, -2.0000],\n",
       "           [-1.6250, -1.9297, -1.9375,  ..., -1.1016, -1.8281, -1.8906]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.5000, -1.5000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.5781, -1.8438, -1.7266,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.5703, -2.0000, -1.9688,  ..., -1.8516, -2.0000, -2.0000],\n",
       "           [-1.3438, -2.0000, -1.9219,  ..., -1.9219, -1.3672, -2.0000],\n",
       "           [-1.5312, -2.0000, -1.9609,  ..., -1.7500, -1.6953, -1.8906]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.8516, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-0.6250,  0.0469, -1.4375,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.2188, -1.9922, -2.0000,  ..., -1.8359, -2.0000, -2.0000],\n",
       "           [-1.0703, -1.9922, -2.0000,  ..., -1.5703, -1.8828, -2.0000],\n",
       "           [-0.6875, -1.9922, -2.0000,  ..., -1.7812, -1.9766, -1.7734]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-0.5156,  0.5156, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-1.7266,  1.5938, -1.8672,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.9453, -2.0000, -2.0000,  ...,  0.5625, -2.0000, -2.0000],\n",
       "           [-1.9766, -2.0000, -2.0000,  ...,  1.6875, -1.8906, -2.0000],\n",
       "           [-1.9297, -2.0000, -2.0000,  ..., -1.6250, -0.3594, -0.2891]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.0938, -1.0859, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.8125, -1.0234, -1.7969,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.4766, -1.9688, -1.9766,  ..., -1.3672, -2.0000, -2.0000],\n",
       "           [-1.0312, -1.9766, -1.9922,  ..., -1.5625, -1.9531, -2.0000],\n",
       "           [-1.9531, -2.0000, -2.0000,  ..., -1.8906,  0.8906, -1.6094]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.9219, -0.9219, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.6562, -0.8359, -1.8281,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.8203, -1.9688, -2.0000,  ..., -1.7734, -2.0000, -2.0000],\n",
       "           [-1.6875, -1.9766, -2.0000,  ..., -1.7969, -1.9453, -2.0000],\n",
       "           [-1.3594, -1.9844, -2.0000,  ..., -1.7109, -1.9375, -1.8359]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9688, -1.9688, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9688, -1.9766, -1.9844,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.7812, -1.9375, -1.9922,  ..., -1.9609, -2.0000, -2.0000],\n",
       "           [ 1.0938, -1.9844, -2.0000,  ..., -1.9844, -1.8906, -2.0000],\n",
       "           [ 0.3906, -1.9375, -1.9922,  ..., -1.9844, -1.6875, -1.9453]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.2031, -1.2031, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6719, -1.8516, -1.8203,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.8594, -1.9766, -1.9766,  ..., -1.9453, -2.0000, -2.0000],\n",
       "           [ 1.1406, -1.9844, -1.9844,  ..., -1.9531, -1.9531, -2.0000],\n",
       "           [ 1.0312, -1.9844, -1.9844,  ..., -1.9688, -1.9609, -1.9688]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.7969, -1.7969, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6250, -1.8125, -1.8125,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.0078, -2.0000, -2.0000,  ..., -1.5781, -2.0000, -2.0000],\n",
       "           [ 1.7500, -2.0000, -2.0000,  ..., -1.9531, -1.9297, -2.0000],\n",
       "           [ 0.8125, -2.0000, -2.0000,  ..., -1.9453, -1.1094, -1.8594]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9688, -1.9609, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.7031, -0.9609, -1.7500,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.7656, -1.2578, -1.9688,  ..., -1.8359, -2.0000, -2.0000],\n",
       "           [-0.3516, -1.4609, -1.9922,  ..., -1.7734, -1.8047, -2.0000],\n",
       "           [-0.6641, -1.4062, -1.9844,  ..., -1.8281, -1.8906, -1.9062]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9844, -1.9766, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9219, -1.9531, -1.9688,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.0000, -1.9766, -2.0000,  ..., -1.9141, -2.0000, -2.0000],\n",
       "           [ 1.8125, -2.0000, -2.0000,  ..., -1.9844, -1.9844, -2.0000],\n",
       "           [ 0.7812, -1.9766, -2.0000,  ..., -1.9844, -1.8594, -1.9609]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8281, -1.8203, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.9375, -1.9062,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.6875, -2.0000, -2.0000,  ..., -1.9219, -2.0000, -2.0000],\n",
       "           [ 1.7500, -2.0000, -2.0000,  ..., -1.9453, -1.9453, -2.0000],\n",
       "           [ 1.6406, -2.0000, -2.0000,  ..., -1.9766, -1.9219, -1.8359]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8906, -1.8906, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8906, -1.9844, -1.9141,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.2656, -1.9609, -1.9297,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.9844, -1.9844, -1.9531,  ..., -2.0000, -1.9141, -2.0000],\n",
       "           [ 1.5781, -1.9922, -1.9844,  ..., -2.0000, -1.9922, -1.9922]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8125, -1.8047, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9688, -1.9609,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.9062, -2.0000, -2.0000,  ..., -1.9844, -2.0000, -2.0000],\n",
       "           [ 1.8594, -2.0000, -2.0000,  ..., -1.9844, -1.8984, -2.0000],\n",
       "           [ 1.9219, -2.0000, -2.0000,  ..., -2.0000, -1.9375, -1.9922]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.8438, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6562, -1.7188, -1.9375,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.2188, -1.6094, -2.0000,  ..., -1.9062, -2.0000, -2.0000],\n",
       "           [ 1.1250, -1.8672, -1.9844,  ..., -1.8906, -1.9375, -2.0000],\n",
       "           [ 1.5625, -1.9766, -2.0000,  ..., -1.9531, -1.9766, -1.9844]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9844, -1.9844, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9688, -2.0000, -1.9766,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.4844, -2.0000, -2.0000,  ..., -1.9922, -2.0000, -2.0000],\n",
       "           [ 1.8594, -2.0000, -2.0000,  ..., -2.0000, -1.9844, -2.0000],\n",
       "           [ 1.9219, -2.0000, -2.0000,  ..., -1.9922, -1.9688, -1.9922]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9062, -1.8984, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6406, -1.8359, -1.7969,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.6250, -2.0000, -2.0000,  ..., -1.8906, -2.0000, -2.0000],\n",
       "           [ 1.5625, -2.0000, -2.0000,  ..., -1.8438, -1.8750, -2.0000],\n",
       "           [ 1.6875, -2.0000, -2.0000,  ..., -1.9453, -1.8281, -1.9531]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.3594, -1.3594, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9453, -1.9922,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.7812, -1.9922, -1.9922,  ..., -1.9922, -2.0000, -2.0000],\n",
       "           [ 1.8281, -2.0000, -2.0000,  ..., -1.9844, -1.9375, -2.0000],\n",
       "           [ 1.8125, -2.0000, -2.0000,  ..., -2.0000, -1.9062, -1.9688]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9844, -1.9766, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9922, -1.9453,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.7500, -2.0000, -1.9922,  ..., -1.9844, -2.0000, -2.0000],\n",
       "           [ 1.7500, -2.0000, -1.9766,  ..., -1.9766, -1.9453, -2.0000],\n",
       "           [ 1.6406, -2.0000, -1.9922,  ..., -1.9609, -1.9375, -1.9609]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.8750, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.5469, -1.9766, -1.5703,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.0000, -1.9375, -1.9922,  ..., -1.4141, -2.0000, -2.0000],\n",
       "           [-0.1172, -1.9922, -2.0000,  ..., -1.9062, -1.7812, -2.0000],\n",
       "           [-0.4609, -1.9922, -1.9922,  ..., -1.8594, -1.9219, -1.1562]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9844, -1.9922, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.9688, -1.8828,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.5625, -2.0000, -1.9844,  ..., -1.9844, -2.0000, -2.0000],\n",
       "           [ 1.7969, -2.0000, -1.9922,  ..., -1.9922, -1.9453, -2.0000],\n",
       "           [ 1.7031, -2.0000, -1.9922,  ..., -1.9844, -1.8828, -1.9766]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9219, -1.9219, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -2.0000, -1.9453,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.3906, -1.9844, -1.9609,  ..., -1.9844, -2.0000, -2.0000],\n",
       "           [ 1.3750, -1.9922, -1.9453,  ..., -1.9844, -1.9688, -2.0000],\n",
       "           [ 1.1562, -1.9922, -1.9688,  ..., -1.9922, -1.9766, -1.9844]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.8750, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9844, -1.9531,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.9062, -1.9922, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8125, -1.9922, -2.0000,  ..., -1.9922, -1.8906, -2.0000],\n",
       "           [ 1.7500, -2.0000, -2.0000,  ..., -1.9922, -1.8672, -1.9688]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9375, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.2812, -1.3438, -1.9375,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.8281, -2.0000, -2.0000,  ..., -1.9688, -2.0000, -2.0000],\n",
       "           [ 0.6406, -2.0000, -2.0000,  ..., -0.9297, -1.9531, -2.0000],\n",
       "           [ 1.4062, -2.0000, -2.0000,  ..., -1.9453, -1.5000, -1.9766]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.8438, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9062, -1.9844, -1.9219,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.4688, -1.9609, -1.8984,  ..., -1.9609, -2.0000, -2.0000],\n",
       "           [-0.5312, -1.9766, -1.9453,  ..., -1.9688, -1.9375, -2.0000],\n",
       "           [-0.9062, -1.9844, -1.9297,  ..., -1.9688, -1.9453, -1.9375]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.8828, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.4062, -1.9062, -1.5000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.2578, -1.9844, -1.9922,  ..., -1.7031, -2.0000, -2.0000],\n",
       "           [-0.7344, -1.9766, -1.9922,  ..., -1.7891, -1.8906, -2.0000],\n",
       "           [-1.1953, -1.9922, -2.0000,  ..., -1.7656, -1.8750, -1.6641]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8906, -1.8828, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.9844, -1.8906,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.9219, -1.9922, -1.9766,  ..., -1.9141, -2.0000, -2.0000],\n",
       "           [ 1.0938, -2.0000, -1.9844,  ..., -1.9922, -1.9766, -2.0000],\n",
       "           [ 0.8125, -2.0000, -1.9922,  ..., -1.9922, -1.9844, -1.9609]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.7969, -1.8047, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.5625, -1.9141, -1.6562,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.5703, -1.9922, -1.9922,  ..., -1.7812, -2.0000, -2.0000],\n",
       "           [-1.2031, -1.9922, -1.9922,  ..., -1.7891, -1.8984, -2.0000],\n",
       "           [-0.7734, -2.0000, -1.9922,  ..., -1.9219, -1.9375, -1.6328]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8594, -1.8516, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.7031, -1.9688, -1.7344,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.2500, -1.9844, -1.9844,  ..., -1.7734, -2.0000, -2.0000],\n",
       "           [ 1.4219, -1.9922, -1.9922,  ..., -1.8984, -1.8516, -2.0000],\n",
       "           [ 1.3125, -1.9922, -1.9922,  ..., -1.9453, -1.8828, -1.9375]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.8281, -0.8281, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.6250, -1.9688, -0.6562,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.8203, -1.9766, -1.9844,  ..., -1.7734, -2.0000, -2.0000],\n",
       "           [-0.6953, -1.9844, -1.9844,  ..., -1.9375, -1.1562, -2.0000],\n",
       "           [-0.7109, -1.9922, -1.9922,  ..., -1.9609, -1.2812, -0.9453]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-2.0000,  2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-2.0000, -2.0000,  2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-2.0000, -2.0000, -2.0000,  ...,  1.8125, -2.0000, -2.0000],\n",
       "           [-2.0000, -2.0000, -2.0000,  ..., -1.9688,  1.8906, -2.0000],\n",
       "           [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.9453,  1.9219]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.1094, -0.1172, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.9531, -1.6875, -1.2656,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.4219, -1.9922, -1.9922,  ..., -1.6250, -2.0000, -2.0000],\n",
       "           [ 0.8438, -1.9844, -1.9922,  ..., -1.7891, -1.2656, -2.0000],\n",
       "           [ 0.9219, -1.9922, -2.0000,  ..., -1.9766, -1.8359, -1.2031]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.2656, -0.2656, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-0.0625, -1.8984, -0.0312,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.0469, -2.0000, -2.0000,  ...,  0.2812, -2.0000, -2.0000],\n",
       "           [-1.1875, -2.0000, -1.9922,  ..., -1.5781,  0.4062, -2.0000],\n",
       "           [-0.8359, -2.0000, -2.0000,  ..., -1.7578, -1.7109, -0.0703]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-0.9531,  0.9531, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.6406, -1.8828, -0.7578,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.0078, -1.9688, -1.9922,  ...,  0.3125, -2.0000, -2.0000],\n",
       "           [-0.4297, -1.9922, -1.9922,  ..., -1.8125, -0.5078, -2.0000],\n",
       "           [ 0.1250, -2.0000, -2.0000,  ..., -1.9219, -1.6719, -0.9844]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.3125, -1.3125, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.4219, -1.8438, -0.5859,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.2656, -2.0000, -1.9922,  ..., -1.8594, -2.0000, -2.0000],\n",
       "           [-1.4844, -2.0000, -1.9844,  ..., -1.8828, -1.9219, -2.0000],\n",
       "           [-1.1328, -2.0000, -2.0000,  ..., -1.9219, -1.9062, -1.0781]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-1.7969,  1.7969, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6719, -1.9141, -1.7500,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.0781, -1.9141, -1.9766,  ..., -0.6562, -2.0000, -2.0000],\n",
       "           [ 0.8281, -1.9922, -1.9844,  ..., -1.8594, -1.4844, -2.0000],\n",
       "           [ 0.4531, -1.9922, -1.9844,  ..., -1.8047, -1.7969, -1.4766]]]],\n",
       "        device='cuda:0', grad_fn=<ToCopyBackward0>),\n",
       " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c71d779e-7b9d-44f0-aa00-ced9abeaae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\560115562.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0271, -0.0674, -0.1068,  ..., -0.0028, -0.1077, -0.1471],\n",
       "        [-0.0835, -0.0467, -0.1045,  ...,  0.0459, -0.1579, -0.1153],\n",
       "        [-0.0839, -0.0469, -0.1067,  ...,  0.0483, -0.1555, -0.1108],\n",
       "        ...,\n",
       "        [-0.0828, -0.0502, -0.1122,  ...,  0.0479, -0.1581, -0.1150],\n",
       "        [-0.0315, -0.0713, -0.1114,  ..., -0.0012, -0.1055, -0.1488],\n",
       "        [-0.0809, -0.0500, -0.0982,  ...,  0.0465, -0.1564, -0.1143]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(training_data.__getitem__(0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5b3e2d-2168-4412-8a8f-acb05d85720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "NUM_CLASSES = 32\n",
    "accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "auroc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b03f3e-f12d-4f60-a303-623267d28fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_1920\\560115562.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [18:00<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 3.5466350113854426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:48<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 3.4660963371451876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:49<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 3.465740100721295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 2: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 2: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:46<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 3.4659348172641424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:33<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 3: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 3: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:50<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 3.4657431532827654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 4: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 4: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:47<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 3.4658019937826006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 5: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 5: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:46<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss: 3.465737959865327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 6: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 6: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:47<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss: 3.4657572952102633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 7: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 7: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:46<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss: 3.4657369651151506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:33<00:00,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 8: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 8: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|██████                                                                | 93/1068 [01:45<16:20,  1.01s/it]"
     ]
    }
   ],
   "source": [
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.squeeze().to(device)\n",
    "        y = y.squeeze().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "        x = x.squeeze().to(device)\n",
    "        y = y.squeeze().to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        # val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    # print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efb1f03d-a7d0-4bf5-99a7-bfec49f6dfa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(1024, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = torchvision.models.convnext_tiny(weights=convnext.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "model = torchvision.models.convnext_tiny()\n",
    "# Change features\n",
    "IN_CHANNELS = 32*32\n",
    "device = 'cuda'\n",
    "model.features[0][0] = nn.Conv2d(IN_CHANNELS, 96, kernel_size=(4,4), stride=(4,4))\n",
    "model.classifier[2] = nn.Linear(768, 2)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff760250-a4fd-4680-90f7-9b846933d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllAttentionMapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_file, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.files.iloc[idx]\n",
    "        pt_path = self.root / item['filename']\n",
    "        heads = torch.load(pt_path)\n",
    "        heads = torch.stack(heads, dim=0).flatten(start_dim=0, end_dim=2)  # 2 is exclusive, only flattens dim0 and dim1\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        return heads.to(torch.float32), torch.tensor(item['prediction'] == item['correct']).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3f71ddf-6103-4d83-b91c-8acfb2e7b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "# dataset = AttentionMapDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_list.txt\", transform=transform)\n",
    "dataset = AllAttentionMapDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_list.txt\", transform=transform)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "split_idx = int(len(indices) * 0.8)\n",
    "training_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "validation_data = torch.utils.data.Subset(dataset, indices[split_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ec5b3f1-4612-4413-960d-aeaa0d7cd54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1068, 267)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "497bfe59-fcd2-4bf6-ad7d-e10d86e8270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=1)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f016950-0650-4604-b66a-003184861ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\4137315560.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  heads = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 74, 74])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b641a899-4904-4259-b8e7-c20e701f16f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\4137315560.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  heads = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3492,  0.3039]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(training_data.__getitem__(0)[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ff780c0-b312-4530-9190-02833f5a8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "accuracy = torchmetrics.Accuracy(task='binary').to(device)\n",
    "recall = torchmetrics.Recall(task='binary').to(device)\n",
    "precision = torchmetrics.Precision(task='binary').to(device)\n",
    "auroc = torchmetrics.AUROC(task='binary').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39a3dc7d-2b4e-44d5-85be-3e07c79f5ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\4137315560.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  heads = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:59<00:00, 17.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 1.2979160130226597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\miniconda3\\envs\\llama-env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "C:\\Users\\sk4835\\miniconda3\\envs\\llama-env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:12<00:00, 20.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 0: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.7849293898534699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:13<00:00, 20.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 1: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:52<00:00, 20.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.7033707734741522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:15<00:00, 17.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 2: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 2: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 2: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.6957453842020214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:17<00:00, 15.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 3: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 3: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 3: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:52<00:00, 20.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.6949262813794033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:19<00:00, 13.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 4: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 4: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 4: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.6937967861971158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:21<00:00, 12.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 5: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 5: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 5: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:53<00:00, 20.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss: 0.6934781480259663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:24<00:00, 11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Accuracy: tensor(0.5056, device='cuda:0')\n",
      "Epoch 6: Validation Precision: tensor(0., device='cuda:0')\n",
      "Epoch 6: Validation Recall: tensor(0., device='cuda:0')\n",
      "Epoch 6: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss: 0.6932987929060218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:26<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Accuracy: tensor(0.5056, device='cuda:0')\n",
      "Epoch 7: Validation Precision: tensor(0., device='cuda:0')\n",
      "Epoch 7: Validation Recall: tensor(0., device='cuda:0')\n",
      "Epoch 7: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:53<00:00, 20.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss: 0.6931857895315363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:28<00:00,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Accuracy: tensor(0.5056, device='cuda:0')\n",
      "Epoch 8: Validation Precision: tensor(0., device='cuda:0')\n",
      "Epoch 8: Validation Recall: tensor(0., device='cuda:0')\n",
      "Epoch 8: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss: 0.6931075874562567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:30<00:00,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Validation Accuracy: tensor(0.5618, device='cuda:0')\n",
      "Epoch 9: Validation Precision: tensor(0.1236, device='cuda:0')\n",
      "Epoch 9: Validation Recall: tensor(0.1236, device='cuda:0')\n",
      "Epoch 9: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y, f)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(validation_dataloader, desc='Validation'):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75fd2c-3e2c-4a7a-ba35-44cd29052f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "test_dataset = AllAttentionMapDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_testset.txt\", transform=transform)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "split_idx = int(len(indices) * 0.8)\n",
    "training_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "validation_data = torch.utils.data.Subset(dataset, indices[split_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c686ce1-539c-4eb8-aa8e-71ad77c53db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_acc = []\n",
    "val_prec = []\n",
    "val_rec = []\n",
    "val_auroc = []\n",
    "for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    logits = model(x)\n",
    "    prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    val_acc.append(accuracy(prediction, y))\n",
    "    val_prec.append(precision(prediction, y))\n",
    "    val_rec.append(recall(prediction, y))\n",
    "    val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
