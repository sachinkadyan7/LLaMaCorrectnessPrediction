{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8fb0b4-a0fd-4489-8b07-acda5f4e0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision.models import convnext\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2001f3-8a56-410d-a76b-3f141a2876d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.convnext_tiny(weights=convnext.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90b045-c306-4aa1-b85e-94f60ed57587",
   "metadata": {},
   "source": [
    "## Experiment: Classify a single layer through all its 32 heads' attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62571827-054e-439c-a3ac-b5afcf741ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "LAYERS_INCLUDED = [1, 11, 21, 31]\n",
    "NUM_LAYER_LLM = len(LAYERS_INCLUDED)\n",
    "\n",
    "class AttentionMapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_file, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files) * NUM_LAYER_LLM\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx = int(idx // NUM_LAYER_LLM)\n",
    "        pt_path = self.root / self.files['filename'][f_idx]\n",
    "        f = torch.load(pt_path)\n",
    "        random_layer = idx % NUM_LAYER_LLM\n",
    "        layer_idx = LAYERS_INCLUDED[random_layer]\n",
    "        heads = f[layer_idx]\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_label = torch.tensor(random_layer)\n",
    "        heads.unsqueeze(dim=0)\n",
    "        bucket_label.unsqueeze(dim=0)\n",
    "        return heads.to(torch.float32), bucket_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2e40c61-cb11-4662-8867-f995531b21c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5552, 1388)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "dataset = AttentionMapDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_list.txt\", transform=transform)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "indices = torch.arange(len(dataset)).tolist()\n",
    "split_idx = int((len(indices) // NUM_LAYER_LLM) * 0.8) * NUM_LAYER_LLM\n",
    "train_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "test_data = torch.utils.data.Subset(dataset, indices[split_idx:])\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a31d705-f1b8-4ae7-b1c1-ed0e507d10ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5552"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "091aa12d-e065-4447-8f1b-230bc6a1c52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_15100\\3130354512.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.5781, -1.5156, -1.8047,  ..., -2.0000, -2.0000,  0.7344],\n",
       "          [-2.0000, -1.0312, -1.6172,  ..., -2.0000, -2.0000,  0.4688],\n",
       "          [-2.0000, -2.0000, -1.6484,  ..., -2.0000, -2.0000,  1.3281],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.6250, -1.8750,  1.5000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.6875,  1.6875],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-0.1953, -1.6719, -1.7812,  ..., -1.9922, -2.0000, -0.8906],\n",
       "          [-2.0000,  0.1875, -1.5859,  ..., -1.9922, -2.0000, -0.9922],\n",
       "          [-2.0000, -2.0000, -0.5781,  ..., -1.9844, -1.9844, -0.3672],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -0.4531, -1.7891,  0.2344],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -0.6172,  0.6250],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.8203, -1.7188, -1.8594,  ..., -1.9844, -1.9688,  0.8594],\n",
       "          [-2.0000, -1.7969, -1.8828,  ..., -1.9766, -1.9766,  1.0781],\n",
       "          [-2.0000, -2.0000, -1.7969,  ..., -1.9609, -1.9609,  1.0938],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.7031, -1.7969,  1.5000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000,  0.9062, -0.9062],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.6875, -1.6406, -1.8125,  ..., -1.9844, -1.9844,  0.2031],\n",
       "          [-2.0000, -1.6875, -1.7969,  ..., -1.9922, -1.9922,  0.8281],\n",
       "          [-2.0000, -2.0000, -1.8203,  ..., -1.9766, -1.9688,  0.6875],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.9688, -1.9844,  1.9531],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.9062,  1.9062],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.9219, -1.9766, -1.9453,  ..., -1.9922, -1.9844,  1.3750],\n",
       "          [-2.0000, -1.9844, -1.9844,  ..., -1.9922, -1.9922,  1.6250],\n",
       "          [-2.0000, -2.0000, -1.9375,  ..., -1.9844, -1.9766,  1.6250],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.9766, -1.9609,  1.9375],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.9141,  1.9062],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-0.7734, -1.2734, -1.8125,  ..., -2.0000, -2.0000, -0.2578],\n",
       "          [-2.0000, -1.1875, -1.9141,  ..., -2.0000, -2.0000,  1.0625],\n",
       "          [-2.0000, -2.0000, -0.8203,  ..., -2.0000, -1.9922,  0.3750],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.0234, -1.9062,  0.9375],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000,  1.1875, -1.1953],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]]],\n",
       "        device='cuda:0'),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[split_idx+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad8b5043-3d2c-4aa5-bec5-b36d38dc5f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_15100\\3130354512.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4219, -1.6875, -1.6875,  ..., -2.0000, -2.0000, -0.4141],\n",
       "          [-2.0000,  0.2344, -1.7344,  ..., -2.0000, -2.0000, -0.6328],\n",
       "          [-2.0000, -2.0000,  0.4531,  ..., -2.0000, -2.0000, -0.7266],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -0.0156, -1.9062, -0.0781],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -0.2422,  0.2500],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.1406, -1.9453, -1.9688,  ..., -2.0000, -2.0000,  0.9531],\n",
       "          [-2.0000, -0.8750, -1.8594,  ..., -2.0000, -2.0000,  0.5938],\n",
       "          [-2.0000, -2.0000, -1.0312,  ..., -2.0000, -2.0000,  0.9062],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.0625, -1.9062,  0.9688],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -0.4766,  0.4688],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[ 0.3750, -1.9844, -1.9922,  ..., -2.0000, -2.0000, -0.8750],\n",
       "          [-2.0000, -1.3594, -1.9844,  ..., -1.9922, -1.9922,  0.6719],\n",
       "          [-2.0000, -2.0000, -1.7969,  ..., -1.9922, -1.9531,  1.2344],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.8828, -1.9141,  1.7969],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000,  0.8906, -0.8906],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.6250, -1.5938, -1.7969,  ..., -2.0000, -2.0000,  0.8438],\n",
       "          [-2.0000, -1.0469, -1.6484,  ..., -2.0000, -2.0000,  0.4531],\n",
       "          [-2.0000, -2.0000, -1.5625,  ..., -2.0000, -2.0000,  1.2344],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.6250, -1.8750,  1.5000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.6875,  1.6875],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-0.8906, -1.6484, -2.0000,  ..., -2.0000, -2.0000,  0.5156],\n",
       "          [-2.0000, -0.4297, -1.9766,  ..., -2.0000, -2.0000,  0.3906],\n",
       "          [-2.0000, -2.0000, -1.2188,  ..., -2.0000, -2.0000,  1.1719],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.5625, -1.9844,  1.5469],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -0.3047,  0.2969],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]],\n",
       " \n",
       "         [[-1.8906, -1.9531, -1.9922,  ..., -2.0000, -2.0000,  1.7969],\n",
       "          [-2.0000, -1.8594, -1.9609,  ..., -2.0000, -2.0000,  1.7656],\n",
       "          [-2.0000, -2.0000, -1.9531,  ..., -2.0000, -2.0000,  1.8594],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.9375, -1.8203,  1.7656],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.8828,  1.8906],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000]]],\n",
       "        device='cuda:0'),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3374f21f-7889-4b6c-992f-199a74864dd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_15100\\3130354512.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000,  2.0000, -2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ...,  2.0000, -2.0000, -2.0000],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000,  1.9531,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [-2.0000,  1.8906, -1.9531,  ..., -2.0000, -2.0000, -2.0000],\n",
       "          [ 1.9844, -1.9844, -2.0000,  ..., -2.0000, -2.0000, -2.0000]],\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -0.1562,  0.1562],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.2656, -1.6953,  0.9688],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -1.7188,  ..., -1.9922, -2.0000,  1.5625],\n",
       "          [-2.0000, -1.1875, -1.8672,  ..., -1.9922, -1.9922,  0.9062],\n",
       "          [-0.9844, -1.9141, -1.9922,  ..., -2.0000, -2.0000,  0.8438]],\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000,  1.1875, -1.1875],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.0156, -1.9141,  0.9219],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -0.8438,  ..., -1.9922, -1.9844,  0.4844],\n",
       "          [-2.0000, -1.1094, -1.8828,  ..., -2.0000, -2.0000,  0.9375],\n",
       "          [-0.8359, -1.1250, -1.7344,  ..., -2.0000, -2.0000, -0.4219]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -0.8516,  0.8594],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.9062, -1.2812,  1.1875],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -1.6250,  ..., -1.9688, -1.9375, -0.2500],\n",
       "          [-2.0000, -1.9141, -1.3594,  ..., -1.9766, -1.9531, -0.5312],\n",
       "          [-1.8750, -1.9688, -1.8359,  ..., -1.9766, -1.9688, -0.0781]],\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.5547,  1.5469],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -1.4922, -1.9219,  1.4062],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000, -1.6406,  ..., -1.9922, -2.0000,  1.4375],\n",
       "          [-2.0000, -1.3125, -1.9219,  ..., -2.0000, -2.0000,  1.0156],\n",
       "          [-0.9688, -1.8750, -1.9609,  ..., -2.0000, -2.0000,  0.5938]],\n",
       " \n",
       "         [[-2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000,  2.0000],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -0.2578,  0.2500],\n",
       "          [-2.0000, -2.0000, -2.0000,  ..., -0.0078, -1.8984, -0.0938],\n",
       "          ...,\n",
       "          [-2.0000, -2.0000,  0.1719,  ..., -2.0000, -2.0000, -0.5000],\n",
       "          [-2.0000,  0.0781, -1.5391,  ..., -2.0000, -2.0000, -0.8203],\n",
       "          [ 0.2656, -1.8359, -1.7812,  ..., -2.0000, -2.0000, -0.9844]]],\n",
       "        device='cuda:0'),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91f0e4ed-1a3b-47e9-b393-60f3923da47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33cc6836-2872-45ec-821d-2262745170bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(32, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change features\n",
    "IN_CHANNELS = 32\n",
    "NUM_CLASSES = NUM_LAYER_LLM\n",
    "device = 'cuda'\n",
    "\n",
    "accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "auroc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "model.features[0][0] = nn.Conv2d(IN_CHANNELS, 96, kernel_size=(4,4), stride=(4,4))\n",
    "model.classifier[2] = nn.Linear(768, NUM_CLASSES)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98980bd8-118b-44b8-81b4-d496e10da442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_15100\\3130354512.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 5552/5552 [02:55<00:00, 31.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 1.5283158838820057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|██████████████████████████████████████████████████████████████████| 1388/1388 [00:24<00:00, 57.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.2500, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.2500, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.2500, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 5552/5552 [02:57<00:00, 31.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 1.5376778817298635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|██████████████████████████████████████████████████████████████████| 1388/1388 [00:23<00:00, 59.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.2500, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.2500, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.2500, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining:   1%|▊                                                                     | 67/5552 [00:02<02:48, 32.51it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y)\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 24\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m nb_tr_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\software\\Miniconda3\\envs\\llama-env\\lib\\site-packages\\torch\\optim\\adam.py:524\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling() \u001b[38;5;129;01mand\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[1;32m--> 524\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    528\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        # val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    # print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28b88c-7c29-46a0-8b62-1429047a8ee5",
   "metadata": {},
   "source": [
    "## Experiment: Classify correctness by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436b9ee-4676-44b1-8ea2-0ab2159d7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "LAYERS_INCLUDED = [1, 11, 21, 31]\n",
    "NUM_LAYER_LLM = len(LAYERS_INCLUDED)\n",
    "\n",
    "class AttentionMapDFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_file, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files) * NUM_LAYER_LLM\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx = int(idx // NUM_LAYER_LLM)\n",
    "        pt_path = self.root / self.files['filename'][f_idx]\n",
    "        f = torch.load(pt_path)\n",
    "        random_layer = idx % NUM_LAYER_LLM\n",
    "        layer_idx = LAYERS_INCLUDED[random_layer]\n",
    "        heads = f[layer_idx]\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_label = torch.tensor(random_layer)\n",
    "        heads.unsqueeze(dim=0)\n",
    "        bucket_label.unsqueeze(dim=0)\n",
    "        return heads.to(torch.float32), bucket_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664d238-0a8b-4733-ac31-7d645e8eb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(Path.home() / \"Downloads/mmlu_attention_files_list.txt\")\n",
    "dataset_df['dataset'] = dataset_df['filename'].apply(lambda filename: str(Path(filename).parent.parent.name))\n",
    "dataset_es = dataset_df[dataset_df['dataset'] == 'elementary_science']\n",
    "\n",
    "dataset_es = AttentionMapDFDataset(Path.home / \"Downloads/mmlu_output/\", dataset_es, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e0ccf-d920-4abf-99d8-44c4a11d5970",
   "metadata": {},
   "source": [
    "## Experiment: Classify correctness by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2615e9ab-3129-4306-bab7-3de44088beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionMapBatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_file, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pt_path = self.root / self.files['filename'][idx]\n",
    "        f = torch.load(pt_path)\n",
    "        heads = torch.stack(f).squeeze()\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        bucket_labels = torch.arange(NUM_LAYER_LLM)\n",
    "        return heads.to(torch.float32), bucket_labels.to(torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11276af7-97b2-4508-a4ee-3951d2ffec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "dataset = AttentionMapBatchDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_list.txt\", transform=transform)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "split_idx = int(len(indices) * 0.8)\n",
    "training_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "test_data = torch.utils.data.Subset(dataset, indices[split_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ea085b-520b-4b9b-afbc-ea5f65eb4461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1068, 267)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0c41c4-d2ff-4b49-999d-7e32bd40a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "824676ff-57fc-4a0c-bc53-613cff267b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\560115562.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6250, -1.6250, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.9062, -1.9688,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.7500, -1.8594, -1.7578,  ..., -1.7266, -2.0000, -2.0000],\n",
       "           [ 0.1562, -1.9766, -1.9922,  ..., -1.8984, -1.9609, -2.0000],\n",
       "           [-1.6250, -1.9297, -1.9375,  ..., -1.1016, -1.8281, -1.8906]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.5000, -1.5000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.5781, -1.8438, -1.7266,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.5703, -2.0000, -1.9688,  ..., -1.8516, -2.0000, -2.0000],\n",
       "           [-1.3438, -2.0000, -1.9219,  ..., -1.9219, -1.3672, -2.0000],\n",
       "           [-1.5312, -2.0000, -1.9609,  ..., -1.7500, -1.6953, -1.8906]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.8516, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-0.6250,  0.0469, -1.4375,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.2188, -1.9922, -2.0000,  ..., -1.8359, -2.0000, -2.0000],\n",
       "           [-1.0703, -1.9922, -2.0000,  ..., -1.5703, -1.8828, -2.0000],\n",
       "           [-0.6875, -1.9922, -2.0000,  ..., -1.7812, -1.9766, -1.7734]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-0.5156,  0.5156, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-1.7266,  1.5938, -1.8672,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.9453, -2.0000, -2.0000,  ...,  0.5625, -2.0000, -2.0000],\n",
       "           [-1.9766, -2.0000, -2.0000,  ...,  1.6875, -1.8906, -2.0000],\n",
       "           [-1.9297, -2.0000, -2.0000,  ..., -1.6250, -0.3594, -0.2891]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.0938, -1.0859, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.8125, -1.0234, -1.7969,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.4766, -1.9688, -1.9766,  ..., -1.3672, -2.0000, -2.0000],\n",
       "           [-1.0312, -1.9766, -1.9922,  ..., -1.5625, -1.9531, -2.0000],\n",
       "           [-1.9531, -2.0000, -2.0000,  ..., -1.8906,  0.8906, -1.6094]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.9219, -0.9219, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.6562, -0.8359, -1.8281,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.8203, -1.9688, -2.0000,  ..., -1.7734, -2.0000, -2.0000],\n",
       "           [-1.6875, -1.9766, -2.0000,  ..., -1.7969, -1.9453, -2.0000],\n",
       "           [-1.3594, -1.9844, -2.0000,  ..., -1.7109, -1.9375, -1.8359]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9688, -1.9688, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9688, -1.9766, -1.9844,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.7812, -1.9375, -1.9922,  ..., -1.9609, -2.0000, -2.0000],\n",
       "           [ 1.0938, -1.9844, -2.0000,  ..., -1.9844, -1.8906, -2.0000],\n",
       "           [ 0.3906, -1.9375, -1.9922,  ..., -1.9844, -1.6875, -1.9453]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.2031, -1.2031, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6719, -1.8516, -1.8203,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.8594, -1.9766, -1.9766,  ..., -1.9453, -2.0000, -2.0000],\n",
       "           [ 1.1406, -1.9844, -1.9844,  ..., -1.9531, -1.9531, -2.0000],\n",
       "           [ 1.0312, -1.9844, -1.9844,  ..., -1.9688, -1.9609, -1.9688]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.7969, -1.7969, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6250, -1.8125, -1.8125,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.0078, -2.0000, -2.0000,  ..., -1.5781, -2.0000, -2.0000],\n",
       "           [ 1.7500, -2.0000, -2.0000,  ..., -1.9531, -1.9297, -2.0000],\n",
       "           [ 0.8125, -2.0000, -2.0000,  ..., -1.9453, -1.1094, -1.8594]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9688, -1.9609, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.7031, -0.9609, -1.7500,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.7656, -1.2578, -1.9688,  ..., -1.8359, -2.0000, -2.0000],\n",
       "           [-0.3516, -1.4609, -1.9922,  ..., -1.7734, -1.8047, -2.0000],\n",
       "           [-0.6641, -1.4062, -1.9844,  ..., -1.8281, -1.8906, -1.9062]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9844, -1.9766, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9219, -1.9531, -1.9688,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.0000, -1.9766, -2.0000,  ..., -1.9141, -2.0000, -2.0000],\n",
       "           [ 1.8125, -2.0000, -2.0000,  ..., -1.9844, -1.9844, -2.0000],\n",
       "           [ 0.7812, -1.9766, -2.0000,  ..., -1.9844, -1.8594, -1.9609]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8281, -1.8203, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.9375, -1.9062,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.6875, -2.0000, -2.0000,  ..., -1.9219, -2.0000, -2.0000],\n",
       "           [ 1.7500, -2.0000, -2.0000,  ..., -1.9453, -1.9453, -2.0000],\n",
       "           [ 1.6406, -2.0000, -2.0000,  ..., -1.9766, -1.9219, -1.8359]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8906, -1.8906, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8906, -1.9844, -1.9141,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.2656, -1.9609, -1.9297,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.9844, -1.9844, -1.9531,  ..., -2.0000, -1.9141, -2.0000],\n",
       "           [ 1.5781, -1.9922, -1.9844,  ..., -2.0000, -1.9922, -1.9922]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8125, -1.8047, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9688, -1.9609,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.9062, -2.0000, -2.0000,  ..., -1.9844, -2.0000, -2.0000],\n",
       "           [ 1.8594, -2.0000, -2.0000,  ..., -1.9844, -1.8984, -2.0000],\n",
       "           [ 1.9219, -2.0000, -2.0000,  ..., -2.0000, -1.9375, -1.9922]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.8438, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6562, -1.7188, -1.9375,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.2188, -1.6094, -2.0000,  ..., -1.9062, -2.0000, -2.0000],\n",
       "           [ 1.1250, -1.8672, -1.9844,  ..., -1.8906, -1.9375, -2.0000],\n",
       "           [ 1.5625, -1.9766, -2.0000,  ..., -1.9531, -1.9766, -1.9844]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9844, -1.9844, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9688, -2.0000, -1.9766,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.4844, -2.0000, -2.0000,  ..., -1.9922, -2.0000, -2.0000],\n",
       "           [ 1.8594, -2.0000, -2.0000,  ..., -2.0000, -1.9844, -2.0000],\n",
       "           [ 1.9219, -2.0000, -2.0000,  ..., -1.9922, -1.9688, -1.9922]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9062, -1.8984, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6406, -1.8359, -1.7969,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.6250, -2.0000, -2.0000,  ..., -1.8906, -2.0000, -2.0000],\n",
       "           [ 1.5625, -2.0000, -2.0000,  ..., -1.8438, -1.8750, -2.0000],\n",
       "           [ 1.6875, -2.0000, -2.0000,  ..., -1.9453, -1.8281, -1.9531]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.3594, -1.3594, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9453, -1.9922,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.7812, -1.9922, -1.9922,  ..., -1.9922, -2.0000, -2.0000],\n",
       "           [ 1.8281, -2.0000, -2.0000,  ..., -1.9844, -1.9375, -2.0000],\n",
       "           [ 1.8125, -2.0000, -2.0000,  ..., -2.0000, -1.9062, -1.9688]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9844, -1.9766, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9922, -1.9453,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.7500, -2.0000, -1.9922,  ..., -1.9844, -2.0000, -2.0000],\n",
       "           [ 1.7500, -2.0000, -1.9766,  ..., -1.9766, -1.9453, -2.0000],\n",
       "           [ 1.6406, -2.0000, -1.9922,  ..., -1.9609, -1.9375, -1.9609]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.8750, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.5469, -1.9766, -1.5703,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.0000, -1.9375, -1.9922,  ..., -1.4141, -2.0000, -2.0000],\n",
       "           [-0.1172, -1.9922, -2.0000,  ..., -1.9062, -1.7812, -2.0000],\n",
       "           [-0.4609, -1.9922, -1.9922,  ..., -1.8594, -1.9219, -1.1562]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9844, -1.9922, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.9688, -1.8828,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.5625, -2.0000, -1.9844,  ..., -1.9844, -2.0000, -2.0000],\n",
       "           [ 1.7969, -2.0000, -1.9922,  ..., -1.9922, -1.9453, -2.0000],\n",
       "           [ 1.7031, -2.0000, -1.9922,  ..., -1.9844, -1.8828, -1.9766]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9219, -1.9219, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -2.0000, -1.9453,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.3906, -1.9844, -1.9609,  ..., -1.9844, -2.0000, -2.0000],\n",
       "           [ 1.3750, -1.9922, -1.9453,  ..., -1.9844, -1.9688, -2.0000],\n",
       "           [ 1.1562, -1.9922, -1.9688,  ..., -1.9922, -1.9766, -1.9844]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.8750, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9844, -1.9531,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.9062, -1.9922, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8125, -1.9922, -2.0000,  ..., -1.9922, -1.8906, -2.0000],\n",
       "           [ 1.7500, -2.0000, -2.0000,  ..., -1.9922, -1.8672, -1.9688]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9375, -1.9375, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.2812, -1.3438, -1.9375,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.8281, -2.0000, -2.0000,  ..., -1.9688, -2.0000, -2.0000],\n",
       "           [ 0.6406, -2.0000, -2.0000,  ..., -0.9297, -1.9531, -2.0000],\n",
       "           [ 1.4062, -2.0000, -2.0000,  ..., -1.9453, -1.5000, -1.9766]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8438, -1.8438, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.9062, -1.9844, -1.9219,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.4688, -1.9609, -1.8984,  ..., -1.9609, -2.0000, -2.0000],\n",
       "           [-0.5312, -1.9766, -1.9453,  ..., -1.9688, -1.9375, -2.0000],\n",
       "           [-0.9062, -1.9844, -1.9297,  ..., -1.9688, -1.9453, -1.9375]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.8828, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.4062, -1.9062, -1.5000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.2578, -1.9844, -1.9922,  ..., -1.7031, -2.0000, -2.0000],\n",
       "           [-0.7344, -1.9766, -1.9922,  ..., -1.7891, -1.8906, -2.0000],\n",
       "           [-1.1953, -1.9922, -2.0000,  ..., -1.7656, -1.8750, -1.6641]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8906, -1.8828, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8750, -1.9844, -1.8906,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.9219, -1.9922, -1.9766,  ..., -1.9141, -2.0000, -2.0000],\n",
       "           [ 1.0938, -2.0000, -1.9844,  ..., -1.9922, -1.9766, -2.0000],\n",
       "           [ 0.8125, -2.0000, -1.9922,  ..., -1.9922, -1.9844, -1.9609]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.7969, -1.8047, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.5625, -1.9141, -1.6562,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.5703, -1.9922, -1.9922,  ..., -1.7812, -2.0000, -2.0000],\n",
       "           [-1.2031, -1.9922, -1.9922,  ..., -1.7891, -1.8984, -2.0000],\n",
       "           [-0.7734, -2.0000, -1.9922,  ..., -1.9219, -1.9375, -1.6328]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.8594, -1.8516, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.7031, -1.9688, -1.7344,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.2500, -1.9844, -1.9844,  ..., -1.7734, -2.0000, -2.0000],\n",
       "           [ 1.4219, -1.9922, -1.9922,  ..., -1.8984, -1.8516, -2.0000],\n",
       "           [ 1.3125, -1.9922, -1.9922,  ..., -1.9453, -1.8828, -1.9375]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.8281, -0.8281, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.6250, -1.9688, -0.6562,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-0.8203, -1.9766, -1.9844,  ..., -1.7734, -2.0000, -2.0000],\n",
       "           [-0.6953, -1.9844, -1.9844,  ..., -1.9375, -1.1562, -2.0000],\n",
       "           [-0.7109, -1.9922, -1.9922,  ..., -1.9609, -1.2812, -0.9453]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-2.0000,  2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-2.0000, -2.0000,  2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-2.0000, -2.0000, -2.0000,  ...,  1.8125, -2.0000, -2.0000],\n",
       "           [-2.0000, -2.0000, -2.0000,  ..., -1.9688,  1.8906, -2.0000],\n",
       "           [-2.0000, -2.0000, -2.0000,  ..., -2.0000, -1.9453,  1.9219]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.1094, -0.1172, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.9531, -1.6875, -1.2656,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 1.4219, -1.9922, -1.9922,  ..., -1.6250, -2.0000, -2.0000],\n",
       "           [ 0.8438, -1.9844, -1.9922,  ..., -1.7891, -1.2656, -2.0000],\n",
       "           [ 0.9219, -1.9922, -2.0000,  ..., -1.9766, -1.8359, -1.2031]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.2656, -0.2656, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-0.0625, -1.8984, -0.0312,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.0469, -2.0000, -2.0000,  ...,  0.2812, -2.0000, -2.0000],\n",
       "           [-1.1875, -2.0000, -1.9922,  ..., -1.5781,  0.4062, -2.0000],\n",
       "           [-0.8359, -2.0000, -2.0000,  ..., -1.7578, -1.7109, -0.0703]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-0.9531,  0.9531, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.6406, -1.8828, -0.7578,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.0078, -1.9688, -1.9922,  ...,  0.3125, -2.0000, -2.0000],\n",
       "           [-0.4297, -1.9922, -1.9922,  ..., -1.8125, -0.5078, -2.0000],\n",
       "           [ 0.1250, -2.0000, -2.0000,  ..., -1.9219, -1.6719, -0.9844]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.3125, -1.3125, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 0.4219, -1.8438, -0.5859,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [-1.2656, -2.0000, -1.9922,  ..., -1.8594, -2.0000, -2.0000],\n",
       "           [-1.4844, -2.0000, -1.9844,  ..., -1.8828, -1.9219, -2.0000],\n",
       "           [-1.1328, -2.0000, -2.0000,  ..., -1.9219, -1.9062, -1.0781]],\n",
       " \n",
       "          [[ 2.0000, -2.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [-1.7969,  1.7969, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           [ 1.6719, -1.9141, -1.7500,  ..., -2.0000, -2.0000, -2.0000],\n",
       "           ...,\n",
       "           [ 0.0781, -1.9141, -1.9766,  ..., -0.6562, -2.0000, -2.0000],\n",
       "           [ 0.8281, -1.9922, -1.9844,  ..., -1.8594, -1.4844, -2.0000],\n",
       "           [ 0.4531, -1.9922, -1.9844,  ..., -1.8047, -1.7969, -1.4766]]]],\n",
       "        device='cuda:0', grad_fn=<ToCopyBackward0>),\n",
       " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c71d779e-7b9d-44f0-aa00-ced9abeaae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\560115562.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0271, -0.0674, -0.1068,  ..., -0.0028, -0.1077, -0.1471],\n",
       "        [-0.0835, -0.0467, -0.1045,  ...,  0.0459, -0.1579, -0.1153],\n",
       "        [-0.0839, -0.0469, -0.1067,  ...,  0.0483, -0.1555, -0.1108],\n",
       "        ...,\n",
       "        [-0.0828, -0.0502, -0.1122,  ...,  0.0479, -0.1581, -0.1150],\n",
       "        [-0.0315, -0.0713, -0.1114,  ..., -0.0012, -0.1055, -0.1488],\n",
       "        [-0.0809, -0.0500, -0.0982,  ...,  0.0465, -0.1564, -0.1143]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(training_data.__getitem__(0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5b3e2d-2168-4412-8a8f-acb05d85720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "NUM_CLASSES = 32\n",
    "accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "auroc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b03f3e-f12d-4f60-a303-623267d28fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_1920\\560115562.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [18:00<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 3.5466350113854426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:48<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 3.4660963371451876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:49<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 3.465740100721295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 2: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 2: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:46<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 3.4659348172641424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:33<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 3: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 3: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:50<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 3.4657431532827654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 4: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 4: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:47<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 3.4658019937826006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 5: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 5: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:46<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss: 3.465737959865327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 6: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 6: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:47<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss: 3.4657572952102633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:34<00:00,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 7: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 7: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [19:46<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss: 3.4657369651151506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:33<00:00,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Accuracy: tensor(0.0312, device='cuda:0')\n",
      "Epoch 8: Validation Precision: tensor(0.0312, device='cuda:0')\n",
      "Epoch 8: Validation Recall: tensor(0.0312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|██████                                                                | 93/1068 [01:45<16:20,  1.01s/it]"
     ]
    }
   ],
   "source": [
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.squeeze().to(device)\n",
    "        y = y.squeeze().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "        x = x.squeeze().to(device)\n",
    "        y = y.squeeze().to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        # val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    # print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efb1f03d-a7d0-4bf5-99a7-bfec49f6dfa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(1024, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = torchvision.models.convnext_tiny(weights=convnext.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "model = torchvision.models.convnext_tiny()\n",
    "# Change features\n",
    "IN_CHANNELS = 32*32\n",
    "device = 'cuda'\n",
    "model.features[0][0] = nn.Conv2d(IN_CHANNELS, 96, kernel_size=(4,4), stride=(4,4))\n",
    "model.classifier[2] = nn.Linear(768, 2)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff760250-a4fd-4680-90f7-9b846933d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllAttentionMapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotations_file, transform=None, target_transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.files = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.files.iloc[idx]\n",
    "        pt_path = self.root / item['filename']\n",
    "        heads = torch.load(pt_path)\n",
    "        heads = torch.stack(heads, dim=0).flatten(start_dim=0, end_dim=2)  # 2 is exclusive, only flattens dim0 and dim1\n",
    "        if self.transform:\n",
    "            heads = self.transform(heads)\n",
    "        return heads.to(torch.float32), torch.tensor(item['prediction'] == item['correct']).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3f71ddf-6103-4d83-b91c-8acfb2e7b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.Normalize([0.5], [0.25]),\n",
    "])\n",
    "\n",
    "# dataset = AttentionMapDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_list.txt\", transform=transform)\n",
    "dataset = AllAttentionMapDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_list.txt\", transform=transform)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "split_idx = int(len(indices) * 0.8)\n",
    "training_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "validation_data = torch.utils.data.Subset(dataset, indices[split_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ec5b3f1-4612-4413-960d-aeaa0d7cd54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1068, 267)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "497bfe59-fcd2-4bf6-ad7d-e10d86e8270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=1)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f016950-0650-4604-b66a-003184861ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\4137315560.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  heads = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 74, 74])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b641a899-4904-4259-b8e7-c20e701f16f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\4137315560.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  heads = torch.load(pt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3492,  0.3039]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(training_data.__getitem__(0)[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ff780c0-b312-4530-9190-02833f5a8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "accuracy = torchmetrics.Accuracy(task='binary').to(device)\n",
    "recall = torchmetrics.Recall(task='binary').to(device)\n",
    "precision = torchmetrics.Precision(task='binary').to(device)\n",
    "auroc = torchmetrics.AUROC(task='binary').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39a3dc7d-2b4e-44d5-85be-3e07c79f5ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\AppData\\Local\\Temp\\ipykernel_13764\\4137315560.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  heads = torch.load(pt_path)\n",
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:59<00:00, 17.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 1.2979160130226597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk4835\\miniconda3\\envs\\llama-env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "C:\\Users\\sk4835\\miniconda3\\envs\\llama-env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:12<00:00, 20.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 0: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 0: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 0: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.7849293898534699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:13<00:00, 20.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 1: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 1: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 1: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:52<00:00, 20.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.7033707734741522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:15<00:00, 17.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 2: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 2: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 2: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.6957453842020214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:17<00:00, 15.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 3: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 3: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 3: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:52<00:00, 20.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.6949262813794033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:19<00:00, 13.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 4: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 4: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 4: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.6937967861971158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:21<00:00, 12.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Accuracy: tensor(0.4944, device='cuda:0')\n",
      "Epoch 5: Validation Precision: tensor(0.4944, device='cuda:0')\n",
      "Epoch 5: Validation Recall: tensor(0.4944, device='cuda:0')\n",
      "Epoch 5: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:53<00:00, 20.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss: 0.6934781480259663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:24<00:00, 11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Accuracy: tensor(0.5056, device='cuda:0')\n",
      "Epoch 6: Validation Precision: tensor(0., device='cuda:0')\n",
      "Epoch 6: Validation Recall: tensor(0., device='cuda:0')\n",
      "Epoch 6: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss: 0.6932987929060218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:26<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Accuracy: tensor(0.5056, device='cuda:0')\n",
      "Epoch 7: Validation Precision: tensor(0., device='cuda:0')\n",
      "Epoch 7: Validation Recall: tensor(0., device='cuda:0')\n",
      "Epoch 7: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:53<00:00, 20.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss: 0.6931857895315363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alidation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:28<00:00,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Accuracy: tensor(0.5056, device='cuda:0')\n",
      "Epoch 8: Validation Precision: tensor(0., device='cuda:0')\n",
      "Epoch 8: Validation Recall: tensor(0., device='cuda:0')\n",
      "Epoch 8: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "raining: 100%|████████████████████████████████████████████████████████████████████| 1068/1068 [00:55<00:00, 19.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss: 0.6931075874562567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 267/267 [00:30<00:00,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Validation Accuracy: tensor(0.5618, device='cuda:0')\n",
      "Epoch 9: Validation Precision: tensor(0.1236, device='cuda:0')\n",
      "Epoch 9: Validation Recall: tensor(0.1236, device='cuda:0')\n",
      "Epoch 9: Validation AUROC: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    for x, y in tqdm(train_dataloader, desc='Training: '):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            logits = model(x)\n",
    "        except Exception as e:\n",
    "            print(x.shape, y, f)\n",
    "            raise e\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss:', tr_loss / nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_auroc = []\n",
    "    for x, y in tqdm(validation_dataloader, desc='Validation'):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        val_acc.append(accuracy(prediction, y))\n",
    "        val_prec.append(precision(prediction, y))\n",
    "        val_rec.append(recall(prediction, y))\n",
    "        val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "    print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "    print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "    print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75fd2c-3e2c-4a7a-ba35-44cd29052f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "test_dataset = AllAttentionMapDataset(Path.home() / \"Downloads/mmlu_output/\", Path.home() / \"Downloads/mmlu_attention_files_testset.txt\", transform=transform)\n",
    "\n",
    "# Randomly split into training and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "split_idx = int(len(indices) * 0.8)\n",
    "training_data = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "validation_data = torch.utils.data.Subset(dataset, indices[split_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c686ce1-539c-4eb8-aa8e-71ad77c53db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_acc = []\n",
    "val_prec = []\n",
    "val_rec = []\n",
    "val_auroc = []\n",
    "for x, y in tqdm(test_dataloader, desc='Validation'):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    logits = model(x)\n",
    "    prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    val_acc.append(accuracy(prediction, y))\n",
    "    val_prec.append(precision(prediction, y))\n",
    "    val_rec.append(recall(prediction, y))\n",
    "    val_auroc.append(auroc(prediction, y))\n",
    "\n",
    "print(f'Epoch {epoch}: Validation Accuracy:', sum(val_acc) / len(val_acc))\n",
    "print(f'Epoch {epoch}: Validation Precision:', sum(val_prec) / len(val_prec))\n",
    "print(f'Epoch {epoch}: Validation Recall:', sum(val_rec) / len(val_rec))\n",
    "print(f'Epoch {epoch}: Validation AUROC:', sum(val_auroc) / len(val_auroc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
